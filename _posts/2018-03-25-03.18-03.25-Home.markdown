---
layout: post
title: "03.18到03.25.home"
author: "Spurs"
date: 2018-03-25 12:40:00
tags:
  - dinary
  - everyday
  - home
---

> 03.18到03.25.home

<!-- more -->

## 03-18

###机器学习中发生过拟合的原因主要有

- 使用了复杂的模型     $\to$ 简化模型假设，或者使用惩罚项限制模型复杂度
- 数据噪声很大 $\to$  进行数据清洗，减少噪声
- 训练数据少 $\to$ 收集更多的数据

###中文分词的基本方法

- 基于语法规则的方法

  > 其基本思想是再分词的同时进行句法、语义分析，利用句法信息和语义信息进行词性标注，以解决分词歧义的现象。因为现有的语法知识、句法规则十分笼统、复杂，基于语法和规则的分析法所能达到的精确到远远还不能令人满意，

- 基于词典的方法

  > 可以分为最大匹配法、最大概率法、最短路径法等。
  >
  > 最大匹配法：按照一定的顺序选取字符串中的若干个字当做一个词，去词典中查找。正向最大匹配，反向最大匹配，双向最大匹配，最小切分。
  >
  > 最大概率法：一个待切分的汉字串可能包含多种分词结果，将其中概率最大的那个作为该字串的分词结果。
  >
  > 最大路径法：次涂上选择一条词数最少的路径。

- 基于统计的方法

  > 根据字符串在语料库中出现的统计频率来决定其是否是一个词，词是字的组合，相邻的字同时出现的次数越多，就越有可能组成一个词。常见的方法有HMM(隐马尔科夫)，MAXENT(最大熵模型)，MEMM(最大熵隐马尔科夫模型)，CRF(随机向量场)

###判别模型和生成模型

- 判别模型通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。

  > 常见的判别模型有线性回归、逻辑回归、支持向量机、传统神经网络、线性判别模型、条件向量场

- 生成模型通过对预测值和标注数据计算联合概率分布P(y,x)来达到判定估算y的目的

  > 常见的生成模型有朴素贝叶斯，隐马尔科夫，贝叶斯网络，隐含狄利克雷分布。

###ID3算法

​	核心思想是以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。ID3算法的局限是它的属性只能取离散值，为了使决策树能应用于连续属性值情况，可以使用ID3的一个扩展算法C4.5算法。BC选项都是ID3算法的特点。ID3算法生成的决策树是一棵多叉树，分支的数量取决于分裂属性有多少个不同的取值。

###HMM模型 

​	对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。

​	CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。

## 03-19

### 熵

条件熵 $H(Y|X) = -\sum P(X,Y)logP(Y|X) = -\sum P(Y|X)P(X)logP(Y|X)$  

交叉熵 $H(X, Y) = -\sum p(X)logY$ 

信息熵 $H(X) = -\sum_x p(X)logp(X)$ 

### Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。

> 寻找到这条最忧直线的准则是Fisher准则：
>
> 两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影结束之后，两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。

### 线性分类器的设计就是利用训练样本建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程。训练过程的共同点就是，先给出准则函数，再寻找使得准则函数趋于极值的优化方法。

###CRF特征灵活，可容纳较多上下文信息，全局最优的结果。但是速度较慢。与HMM和MEMM相比较而言。

###HMM中

- EM算法：只有观测序列，无状态序列是学习模型参数，即Baum_Welch算法。
- 维特比算法：用动态规划的方法来解决HMM的预测问题，不是参数估计
- 前向后向算法：用来计算概率
- 极大似然估计：即观测序列和相应的状态序列同时存在时候，的监督算法。用来估计参数

###KL变换与PCA变换是不同的概念，PCA的变化矩阵是协方差矩阵，KL变换的矩阵可以有多种（二阶矩阵，协方差矩阵，总类内离散度矩阵等等）。当KL变换矩阵为协方差矩阵时候，等同于PCA。

###Logit回归本质上是一种使用样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积，logit最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率，可以用于预测时间发生的概率的大小。SVM的目标是结构风险最小化，可以有效避免模型过拟合。

###贝叶斯分类的目标函数是最小化后验概率

###线性分类器有三大类，分别是感知器准则函数，SVM，Fisher准则，而贝叶斯分类不是线性分类器

- 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。
- 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）
- Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
  ​       根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现

###在统计模式分类问题中，当先验概率未知时，可以使用最小最大损失准则

###防止过拟合的方法

- 过拟合的一个原因是：模型的学习能力太强了，
  1. earlystopping：早停止，如果在训练中发现模型性能没有明显提高，就可以提前结束模型训练
  2. 正则化：可以限制模型复杂度
- 另一个原因是数据样本太不理想了：
  1. 增加样本数量
  2. 特征工程，特征降维，提高特征之间的独立性
- 模型训练方法
  1. 交叉验证
  2. dropout随机失活

###CRF相对于HMM、MEMM的优势和缺点

- CRF、HMM、MEMM都常用来对序列标注
- HMM的最大确定是输入的独立性假设，导致其不能利用上下文的特征，限制的特征的选择
- MEMM解决了HMM的问题，可以随机的选择特征，但由于每个节点都需要进行归一化，所以只能选出局部的最优值，同时也带来了标记偏见的问题，即凡是在训练语料中未出现的情况都会被忽略掉
- 条件随机场很好的解决了这一问题，他不在每个节点上对一化，而是所有特征进行全局归一化，所以可以求出全局最优值。


- 熵是随机变量不确定性的度量，不确定性越大，熵越大。
- **了解正则化吗**
  - 正则化是针对过拟合而提出的，以前在求解模型最优的时候一般优化最小的经验风险，现在在该经验风险的基础上加入了模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来衡量模型复杂度与经验风险的权重。
  - 如果模型复杂度越高，结构化的经验风险就会越大，现在的目标就变成了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低了过拟合的风险。
  - 奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。
- **协方差和相关性的区别**

  - 相关性是协方差的标准化格式，协方差本身很难做比较。例如：如果我们要计算工资和年龄的协方差，因为这两个变量有不同的度量，所以我们会得到不同的协方差
  - 为了解决这一问题，我们计算相关性来得到一个介于-1～+1之间的值，就可以忽略他们各自的不同的度量

## 03-20

###Hash函数冲突及解决办法

- 关键字值不同的元素可能会影响到哈希表的同一地址而发生冲突。
- 开放定址法：当冲突发生时，使用某种探查技术在此序列中逐个单元的查找，知道找到给定的关键字，或者碰到一个开放的地址为止
- 再哈希法：同时构造多个不同的哈希函数
- 链地址法：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在的哈希表的第i个单元中，因而查找/插入和删除主要在同义词链中进行。链地址法适用于经常插入和删除的情况。
- 建立公共溢出区，将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表中。

###线性分类器和非线性分类器的区别和优劣

- 线性和非线性是针对模型参数和输入特征来讲的，比如输入x，模型y=ax + ax^2那么就是非线性模型，如果输入是x和x^2则模型是线性的。
- 线性分类器的可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对较弱。
- 非线性分类器的拟合效果较强，不足之处是数据量不足时容易过拟合、计算复杂度高、可解释性不好。
- 常见的线性分类器有：LR、贝叶斯分类器、单层感知机、线性回归
- 常见的非线性分类器：决策树、RF、GBDT、多层感知机
- SVM两种都有。

###数据的逻辑存储结构（如数组、队列、树等），对于软件开发具有十分重要的影响，那么各种存储结构的从运行速度、存储效率和适用场合等方面简要分析。

|      | 运行速度 | 存储效率 | 适用场合                                     |
| ---- | ---- | ---- | ---------------------------------------- |
| 数组   | 快    | 高    | 比较适合进行查找操作，还有像类似于矩阵等的操作                  |
| 链表   | 较快   | 较高   | 比较适合增删改频繁的操作，动态的分配内存                     |
| 队列   | 较快   | 较高   | 比较适合进行任务类等的调度                            |
| 栈    | 一般   | 较高   | 比较适合递归类程序的改写                             |
| 二叉树  | 较快   | 一般   | 一切具有层次关系的问题都可用树来描述                       |
| 图    | 一般   | 一般   | 除了最小生成树、最短路径、拓扑排序等经典用途之外，还有用于像神经网络等人工智能等领域。 |
| 字典树  | 堆    | 哈希表  | 哈希set                                    |

###说一下贝叶斯定理

- 后验概率(条件概率)就是事件A在另外一个事件B已经发生的条件下的发生概率。$P(A|B)$ 读作“在B条件下A发生的概率”
- 联合概率就是两个事件同时发生的概率
- 先验概率(边缘概率)就是某个事件发生的概率。
- $P(A|B) = P(A)P(B|A) / P(B)$ 就是贝叶斯公式

###```#include```和`#include "filename.h"`有什么区别？

- 用`#include`格式来引用标准哭的头文件（编译器将从标准库目录开始搜素）
- 用`#include "filename.h"`格式来引用非标准库的头文件（编译器会从用户的工作目录开始搜索）

###小问题

- 啤酒和尿布的数据发现，属于数据挖掘中的那类问题？关联规则发现
- 将原始数据进行集成、变换、纬度规约、数值规约是在以下那个步骤的任务？数据预处理
- 数据预处理的方法：变量代换、离散化、聚类
- 什么事KDD？数据挖掘和知识发现
- 当不知道数据所带标签时，可以使用那种技术促进带同类表圈的数据与带其他表圈的数据相分离？聚类
- 建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的那一类任务？预测建模
- 特征选择的标准方法：嵌入、过滤、包装

###简单说一下$sigmoid$激活函数

- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/sigmoid.png)
- 常见的非线性激活函数$sigmoid、tanh、relu$等，前两者常见于全连接层，后者常见于卷积层 
- $sigmoid$函数的功能相当于把一个实数压缩到0-1之间。当z是非常大的正数时，$g(z)$会趋近于1，而z是非常小的负数时，则$g(z)$是会趋近于0.
- 压缩的好处是可以将激活函数看作是一种“分类的概率”。
- 不足之处是$sigmoid$函数，在bp形式传递loss时候容易造成梯度弥散。

###什么是卷积

- 对图像矩阵（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看作一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的$卷积$操作，也就是卷积神经网络的名字的来源。

###说说梯度下降算法

- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/machinelearning.png)
- 上面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。
- 建立模型的过程中涉及到了目标函数，损失函数，梯度求解过程。
- 梯度下降法的流程如下：
  - 首先对$\theta$赋值，随机的，
  - 改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。

###梯度下降法找到一定是下降最快的方向么？

- 梯度下降法并不是下降最快的方向，它只是目标函数在当前点的切平面上下降最快的方向，在practical implement中，牛顿方法(考虑海森矩阵)才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的。
- 一般在解释梯度下降法时候，会用下山来举例。
- 梯度上升来求函数的最大值，梯度下降求最小值。
- 在ML中，基于基本的梯度下降法发展出了两种梯度下降法，分别是随机梯度下降法和批量梯度下降法。
  - 普通的随机梯度下降法：在更新回归系数的时候要遍历整个数据集，是一种批处理方法，这样数据特别庞大时候，可能出现以下问题：
    - 收敛过程可能非常慢
    - 如果存在多个局部极小值，很难保证找到全局最小值
  - 随机梯度下降法：根据每个单独的训练样本来更新权值
  - 批量梯度下降算法：不是整体的样本而是随机选择m个样本来进行梯度更新。
- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/gradient_descent.png)

## 03-21

###牛顿法和梯度下降法有什么不同

- 牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数$f(x)$的泰勒级数的前面几项来寻找方程$f(x)=0$的根。牛顿法最大的特点在于它的收敛速度很快。

  1. 选择一个接近函数$f(x)$零点的$x_0$，计算相应的$f(x_0)$和切线斜率$f{'}(x_0)$。然后我们计算穿过点$(x_0, f(x_0))$并且斜率为$f{'}(x_0)$的直线和x轴的交点的x坐标，也就是求如下方程的解：

     ​			$x * f{'}(x_0) + f(x_0) - x_0 * f{'}(x_0) = 0$

     所求的点的x坐标命名为$x_1$，通常x1会比x0更接近于方程$f(x) = 0$的解。因此我们现在可以利用$x_1$开始下一轮迭代。迭代公式可简化为如下所示：

     ​			$x_{n+1} = x_n - f(x_n)/f{'}(x_n)$

     可以证明：牛顿法具有平方收敛的性能，这意味着，每次迭代，牛顿法结果的有效数字将增加一倍

     由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被形象的称为“切线法”。

- 关于牛顿法和梯度下降法的效率对比

  - 从收敛速度上来看，牛顿法是二阶收敛，梯度下降是一阶收敛，牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是从局部上看的更细致，梯度法仅仅考虑了方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。
  - 根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的下降路径。

- 牛顿法的优缺点总结：

  - 优点：二阶收敛，收敛速度快。
  - 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hession矩阵的逆矩阵，计算比较复杂。

### 2.什么是拟牛顿法？

- 拟牛顿法是求解非线性优化问题最有效的方法之一，其本质是改善牛顿法每次需要求解复杂的Hession矩阵的逆矩阵的缺陷，他使用正定矩阵来近似Hession矩阵的逆，从而简化运算的复杂度。
- 拟牛顿法和最速下降法一样只要求每一步迭代事知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。

## 03-22

###对于所有的优化问题，有没有可能找到比现在已知算法更好的算法？

- 没有免费的午餐定理：
  - 对于训练样本，不同算法A/B在不同的测试样本中有不同的表现，这表示：对于一个算法A，若他在某些问题上比学习算法B更好，则必然存在一些问题，在哪里B比A好。
  - 也就是说，对于所有问题，无论学习算法A多聪明，学习算法B多笨拙，他们的期望性能相同。但是：没有免费午餐定理假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布。**所以，在优化算法时，针对具体问题进行分析，是算法法优化的核心所在。**

###Python到底是什么样的语言？你可以比较其他技术或者语言来回答你的问题

- Python是解释型语言，这意味着不像C语言和其他语言，Python运行之前不需要编译。其他解释型语言包括PHP和Ruby
- Python是动态类型的，这意味着你不需要在声明变量是指定类型。
- Python是面向对象语言，所有允许定义类并且可以继承和组合。Python没有访问标示public、private
- Python 中函数是一等公民。这意味着他们可以被赋值，从其他函数返回值，并且传递函数对象。
- 写Python很快，但是跑起来会比编译型语言慢。幸福的是，Python允许使用C扩展写程序，Numpy就是一个很好的例子，因为很多代码不是Python 直接写的，所以运行很快。
- Python使用场景很多-Web应用开发、大数据应用、数据科学、人工智能等。它也经常被看作“胶水”语言，使得不同语言可以衔接上。
- Python可以简化工作，使得程序员能够关心如何重写代码而不是详细看一遍底层实现。

###Python是如何进行内存管理的？

> 三个方面来说，分别是对象的引用计数，垃圾回收机制，内存池机制

- 对象的引用计数
  - Python内部使用引用计数，来保持追踪内存中的对象，所有对象都有引用计数
  - 计数增加的情况：
    - 一个对象分配一个新名称
    - 将其放入一个容器中（如列表、元祖、或字典）
  - 计数减少的情况：
    - 使用del语句对对象别名显示的销毁
    - 引用超出作用域或者被重新赋值
  - 多数情况下，引用计数会比你猜测的要大得多。对于不可变数据（如数字和字符串），解释器会在程序的不同部分共享内存，以便节约内存。
- 垃圾回收
  - 当一个对象的引用计数归零时，它将被垃圾回收机制处理掉
  - 当两个对象A和B相会引用时，del语句可以减少A和B的引用计数，并销毁用于引用底层对象的名称。然而由于每个对象都包含一个对其他对象的引用，因此引用计数不会归零，对象也不会销毁。**（从而导致内存泄露）**。为解决问题，解释器会定期执行一个循环检测器，搜索不可访问对象的循环并删除他们。
- 内存池机制
  - Python提供了对内存的垃圾回收机制，但是他将不用的内存放到内存池而不是返回给操作系统。
  - Pymalloc机制，为了加速Python的执行效率，Python引入了内存池机制，用于管理对小块内存的申请和释放。
  - Python中所有小于256个字节的对象都是使用Pymalloc实现的分配器，而大的对象则使用系统的malloc。
  - 对于Python对象，如整数，浮点数和list，都有其私有内存池，对象间不共享他们的内存池。也就是如果你分配又释放了大量的整数，用于缓存这些整数的内存就不能再分配给浮点数。

###Python程序删除list中重复的元素

- set(list)

- ```python
  a = [1,2,3,3,4,5]
  b = {}
  b = b.fromkeys(a)
  c = list(b.keys)()
  ```

- 排序后检测

###Python中如何产生随机数

- ```python
  random模块
  random.randint(a, b)
  random.randrange(start, stop, [,step])
  random.random()
  random.uniform(a, b)
  ```

## 03-23

## 03-24

## END

