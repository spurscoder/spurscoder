---
layout: post
title: "home"
author: "Spurs"
date: 2018-06-03 12:40:00
description:
photos:
links:
categories:
tags:
---

> home
>

<!-- more -->

#### 05.23

##### tensorflow 中模型的保存与加载

我们在上线使用一个算法模型的时候，首先必须将已经训练好的模型保存下来。tensorflow保存模型的方式与sklearn不太一样，sklearn很直接，一个sklearn.externals.joblib的dump与load方法就可以保存与载入使用。而tensorflow由于有graph, operation 这些概念，保存与载入模型稍显麻烦。

一、基本方法

网上搜索tensorflow模型保存，搜到的大多是基本的方法。即

保存

1. 定义变量
2. 使用saver.save()方法保存

载入

1. 定义变量
2. 使用saver.restore()方法载入

如 **保存** 代码如下

```
import tensorflow as tf  
import numpy as np  

W = tf.Variable([[1,1,1],[2,2,2]],dtype = tf.float32,name='w')  
b = tf.Variable([[0,1,2]],dtype = tf.float32,name='b')  

init = tf.initialize_all_variables()  
saver = tf.train.Saver()  
with tf.Session() as sess:  
        sess.run(init)  
        save_path = saver.save(sess,"save/model.ckpt")
```

**载入**代码如下

```
import tensorflow as tf  
import numpy as np  

W = tf.Variable(tf.truncated_normal(shape=(2,3)),dtype = tf.float32,name='w')  
b = tf.Variable(tf.truncated_normal(shape=(1,3)),dtype = tf.float32,name='b')  

saver = tf.train.Saver()  
with tf.Session() as sess:  
        saver.restore(sess,"save/model.ckpt")
```

这种方法不方便的在于，在使用模型的时候，必须把模型的结构重新定义一遍，然后载入对应名字的变量的值。但是很多时候我们都更希望能够读取一个文件然后就直接使用模型，而不是还要把模型重新定义一遍。所以就需要使用另一种方法。

二、不需重新定义网络结构的方法

tf.train.import_meta_graph

```
import_meta_graph(
    meta_graph_or_file,
    clear_devices=False,
    import_scope=None,
    **kwargs
)
```

这个方法可以从文件中将保存的graph的所有节点加载到当前的default graph中，并返回一个saver。也就是说，我们在保存的时候，除了将变量的值保存下来，其实还有将对应graph中的各种节点保存下来，所以模型的结构也同样被保存下来了。

比如我们想要保存计算最后预测结果的`y`，则应该在训练阶段将它添加到collection中。具体代码如下

保存

```
### 定义模型
input_x = tf.placeholder(tf.float32, shape=(None, in_dim), name='input_x')
input_y = tf.placeholder(tf.float32, shape=(None, out_dim), name='input_y')

w1 = tf.Variable(tf.truncated_normal([in_dim, h1_dim], stddev=0.1), name='w1')
b1 = tf.Variable(tf.zeros([h1_dim]), name='b1')
w2 = tf.Variable(tf.zeros([h1_dim, out_dim]), name='w2')
b2 = tf.Variable(tf.zeros([out_dim]), name='b2')
keep_prob = tf.placeholder(tf.float32, name='keep_prob')
hidden1 = tf.nn.relu(tf.matmul(self.input_x, w1) + b1)
hidden1_drop = tf.nn.dropout(hidden1, self.keep_prob)
### 定义预测目标
y = tf.nn.softmax(tf.matmul(hidden1_drop, w2) + b2)
# 创建saver
saver = tf.train.Saver(...variables...)
# 假如需要保存y，以便在预测时使用
tf.add_to_collection('pred_network', y)
sess = tf.Session()
for step in xrange(1000000):
    sess.run(train_op)
    if step % 1000 == 0:
        # 保存checkpoint, 同时也默认导出一个meta_graph
        # graph名为'my-model-{global_step}.meta'.
        saver.save(sess, 'my-model', global_step=step)
```

载入

```
with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
  new_saver.restore(sess, 'my-save-dir/my-model-10000')
  # tf.get_collection() 返回一个list. 但是这里只要第一个参数即可
  y = tf.get_collection('pred_network')[0]

  graph = tf.get_default_graph()

  # 因为y中有placeholder，所以sess.run(y)的时候还需要用实际待预测的样本以及相应的参数来填充这些placeholder，而这些需要通过graph的get_operation_by_name方法来获取。
  input_x = graph.get_operation_by_name('input_x').outputs[0]
  keep_prob = graph.get_operation_by_name('keep_prob').outputs[0]

  # 使用y进行预测  
  sess.run(y, feed_dict={input_x:....,  keep_prob:1.0})
```

**这里有两点需要注意的：** 
一、 saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 
`my-model-10000.meta`, `my-model-10000.index`, `my-model-10000.data-00000-of-00001` 
在`import_meta_graph`时填的就是meta文件名，我们知道权值都保存在`my-model-10000.data-00000-of-00001`这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是**前缀**，这个前缀可以使用`tf.train.latest_checkpoint(checkpoint_dir)`这个方法获取。

二、模型的y中有用到placeholder，在sess.run()的时候肯定要feed对应的数据，因此还要根据具体placeholder的名字，从graph中使用`get_operation_by_name`方法获取。

##### pip freeze 用途

Output installed packages in requirements format.

packages are listed in a case-insensitive sorted order.

```
$ env1/bin/pip freeze > requirements.txt
$ env2/bin/pip install -r requirements.txt
```

> .freeze显示的只有17个，而pip list 和 pip freeze --all显示了20个，
> freeze 显示 少了 pip , wheel, setuptools.
=======
#### 06.03

##### 2018腾讯社交广告大赛

>腾讯社交广告Lookalike相似人群拓展，原理是基于目标人群，从海量其他人群中寻找相似人群，并拓展人群规模。
>
>实际业务中，Lookalike能给予广告主已有的消费者，找出与已有消费者相似的潜在消费者，以帮助广告主挖掘新客，拓展业务

1. 特征工程：

   用户特征： interest五个，kw三个，topic三个，教育年龄，性别，有无房子，婚姻状态等用户特征

   广告特征：广告主id，广告推广计划，创建时间，产品id，产品类型等特征。

   训练集中：其中正例有42万，而反例有800万左右。

   用户：有userfeature.data 1100万，其中800万的训练集，200万的测试集合

   评价标准是所有待评估的m个种子包的平均AUC值。

   1. 首先处理缺失值，对计算缺失率，考虑将缺失率较大的特征直接删除，发现删除之后模型效果更好了，

      全部是类别型特征，用的OntHotEncoder()，以及CountVectorizer()

   2. 其中有一个特征直接是26万中取值，特征维数大约为40万。

      剔除掉缺失率过高的特征: interest3, interest4, kw3, appIdInstall, topic3；去除特征维数比较大的特征之后，效果提升

   3. 通过绘制柱状图，查看每个特征与目标之间的关系，查看特征之间的关系

   4. 联网类型可能会影响广告转化率，推广计划转化率特征，用户所在位置的历史点击率特征

   5. 广告转化率，发出的广告且别发送给种子用户的数量

   6. 对特征进行了归一化

   7. 点击某广告的用户平均年龄；用户的广告点击次数，广告点击率；分析广告投放以及地理位置的关系

   8. 用户的兴趣和广告的兴趣有多少相符合；用户的兴趣总数；特征年龄分布

   9. clf.feature_importances；模型调参数，重点调节estimators，learning_rate, grid_search

   10. 尝试使用embedding来进行降维interest kw topic

   11. 16G内存

   12. 随机下采样负样例，训练多个LGB之后，模型融合，现实加权融合，之后bagging，

       因为下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。

       上采样容易过拟合

2. 1、**Bagging**：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。  2、**Boosting**：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。  3、**Blending**：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。  4、**Stacking**：接下来会详细介绍。 

3. https://blog.csdn.net/u012969412/article/details/76636336

   stacking是不重复的，而blending是不重复的。

   stacking就是训练出一个权重来，你和多个level1模型的结果，最终得到真正的结果。

   bagging：使用训练集的不同随机子集来训练每个Base model，最后进行每个basemodel权重相同的Vote，也即Random Forest的原理。

   可以使用stacking的第一层的方式训练模型。分为n

   在实际比赛中往往不能单纯的使用某一种模型融合方法。
#### 06-07

##### 什么是长短时依赖long short term dependency.
















#### end

