---
layout: post
title: "home"
author: "Spurs"
date: 2018-06-03 12:40:00
description:
photos:
links:
categories:
tags:
---

> home
>

<!-- more -->

#### 05.23

##### tensorflow 中模型的保存与加载

我们在上线使用一个算法模型的时候，首先必须将已经训练好的模型保存下来。tensorflow保存模型的方式与sklearn不太一样，sklearn很直接，一个sklearn.externals.joblib的dump与load方法就可以保存与载入使用。而tensorflow由于有graph, operation 这些概念，保存与载入模型稍显麻烦。

一、基本方法

网上搜索tensorflow模型保存，搜到的大多是基本的方法。即

保存

1. 定义变量
2. 使用saver.save()方法保存

载入

1. 定义变量
2. 使用saver.restore()方法载入

如 **保存** 代码如下

```
import tensorflow as tf  
import numpy as np  

W = tf.Variable([[1,1,1],[2,2,2]],dtype = tf.float32,name='w')  
b = tf.Variable([[0,1,2]],dtype = tf.float32,name='b')  

init = tf.initialize_all_variables()  
saver = tf.train.Saver()  
with tf.Session() as sess:  
        sess.run(init)  
        save_path = saver.save(sess,"save/model.ckpt")
```

**载入**代码如下

```
import tensorflow as tf  
import numpy as np  

W = tf.Variable(tf.truncated_normal(shape=(2,3)),dtype = tf.float32,name='w')  
b = tf.Variable(tf.truncated_normal(shape=(1,3)),dtype = tf.float32,name='b')  

saver = tf.train.Saver()  
with tf.Session() as sess:  
        saver.restore(sess,"save/model.ckpt")
```

这种方法不方便的在于，在使用模型的时候，必须把模型的结构重新定义一遍，然后载入对应名字的变量的值。但是很多时候我们都更希望能够读取一个文件然后就直接使用模型，而不是还要把模型重新定义一遍。所以就需要使用另一种方法。

二、不需重新定义网络结构的方法

tf.train.import_meta_graph

```
import_meta_graph(
    meta_graph_or_file,
    clear_devices=False,
    import_scope=None,
    **kwargs
)
```

这个方法可以从文件中将保存的graph的所有节点加载到当前的default graph中，并返回一个saver。也就是说，我们在保存的时候，除了将变量的值保存下来，其实还有将对应graph中的各种节点保存下来，所以模型的结构也同样被保存下来了。

比如我们想要保存计算最后预测结果的`y`，则应该在训练阶段将它添加到collection中。具体代码如下

保存

```
### 定义模型
input_x = tf.placeholder(tf.float32, shape=(None, in_dim), name='input_x')
input_y = tf.placeholder(tf.float32, shape=(None, out_dim), name='input_y')

w1 = tf.Variable(tf.truncated_normal([in_dim, h1_dim], stddev=0.1), name='w1')
b1 = tf.Variable(tf.zeros([h1_dim]), name='b1')
w2 = tf.Variable(tf.zeros([h1_dim, out_dim]), name='w2')
b2 = tf.Variable(tf.zeros([out_dim]), name='b2')
keep_prob = tf.placeholder(tf.float32, name='keep_prob')
hidden1 = tf.nn.relu(tf.matmul(self.input_x, w1) + b1)
hidden1_drop = tf.nn.dropout(hidden1, self.keep_prob)
### 定义预测目标
y = tf.nn.softmax(tf.matmul(hidden1_drop, w2) + b2)
# 创建saver
saver = tf.train.Saver(...variables...)
# 假如需要保存y，以便在预测时使用
tf.add_to_collection('pred_network', y)
sess = tf.Session()
for step in xrange(1000000):
    sess.run(train_op)
    if step % 1000 == 0:
        # 保存checkpoint, 同时也默认导出一个meta_graph
        # graph名为'my-model-{global_step}.meta'.
        saver.save(sess, 'my-model', global_step=step)
```

载入

```
with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
  new_saver.restore(sess, 'my-save-dir/my-model-10000')
  # tf.get_collection() 返回一个list. 但是这里只要第一个参数即可
  y = tf.get_collection('pred_network')[0]

  graph = tf.get_default_graph()

  # 因为y中有placeholder，所以sess.run(y)的时候还需要用实际待预测的样本以及相应的参数来填充这些placeholder，而这些需要通过graph的get_operation_by_name方法来获取。
  input_x = graph.get_operation_by_name('input_x').outputs[0]
  keep_prob = graph.get_operation_by_name('keep_prob').outputs[0]

  # 使用y进行预测  
  sess.run(y, feed_dict={input_x:....,  keep_prob:1.0})
```

**这里有两点需要注意的：** 
一、 saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 
`my-model-10000.meta`, `my-model-10000.index`, `my-model-10000.data-00000-of-00001` 
在`import_meta_graph`时填的就是meta文件名，我们知道权值都保存在`my-model-10000.data-00000-of-00001`这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是**前缀**，这个前缀可以使用`tf.train.latest_checkpoint(checkpoint_dir)`这个方法获取。

二、模型的y中有用到placeholder，在sess.run()的时候肯定要feed对应的数据，因此还要根据具体placeholder的名字，从graph中使用`get_operation_by_name`方法获取。

##### pip freeze 用途

Output installed packages in requirements format.

packages are listed in a case-insensitive sorted order.

```
$ env1/bin/pip freeze > requirements.txt
$ env2/bin/pip install -r requirements.txt
```

> .freeze显示的只有17个，而pip list 和 pip freeze --all显示了20个，
>
> freeze 显示 少了 pip , wheel, setuptools.
#### 06.03

##### 2018腾讯社交广告大赛

>腾讯社交广告Lookalike相似人群拓展，原理是基于目标人群，从海量其他人群中寻找相似人群，并拓展人群规模。
>
>实际业务中，Lookalike能给予广告主已有的消费者，找出与已有消费者相似的潜在消费者，以帮助广告主挖掘新客，拓展业务

1. 特征工程：

   用户特征： interest五个，kw三个，topic三个，教育年龄，性别，有无房子，婚姻状态等用户特征

   广告特征：广告主id，广告推广计划，创建时间，产品id，产品类型等特征。

   训练集中：其中正例有42万，而反例有800万左右。

   用户：有userfeature.data 1100万，其中800万的训练集，200万的测试集合

   评价标准是所有待评估的m个种子包的平均AUC值。

   1. 首先处理缺失值，对计算缺失率，考虑将缺失率较大的特征直接删除，发现删除之后模型效果更好了，

      全部是类别型特征，用的OntHotEncoder()，以及CountVectorizer()

   2. 其中有一个特征直接是26万中取值，特征维数大约为40万。

      剔除掉缺失率过高的特征: interest3, interest4, kw3, appIdInstall, topic3；去除特征维数比较大的特征之后，效果提升

   3. 通过绘制柱状图，查看每个特征与目标之间的关系，查看特征之间的关系

   4. 联网类型可能会影响广告转化率，推广计划转化率特征，用户所在位置的历史点击率特征

   5. 广告转化率，发出的广告且别发送给种子用户的数量

   6. 对特征进行了归一化

   7. 点击某广告的用户平均年龄；用户的广告点击次数，广告点击率；分析广告投放以及地理位置的关系

   8. 用户的兴趣和广告的兴趣有多少相符合；用户的兴趣总数；特征年龄分布

   9. clf.feature_importances；模型调参数，重点调节estimators，learning_rate, grid_search

   10. 尝试使用embedding来进行降维interest kw topic

   11. 16G内存

   12. 随机下采样负样例，训练多个LGB之后，模型融合，现实加权融合，之后bagging，

       因为下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。

       上采样容易过拟合

2. 1、**Bagging**：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。  2、**Boosting**：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。  3、**Blending**：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。  4、**Stacking**：接下来会详细介绍。 

3. https://blog.csdn.net/u012969412/article/details/76636336

   stacking是不重复的，而blending是不重复的。

   stacking就是训练出一个权重来，你和多个level1模型的结果，最终得到真正的结果。

   bagging：使用训练集的不同随机子集来训练每个Base model，最后进行每个basemodel权重相同的Vote，也即Random Forest的原理。

   可以使用stacking的第一层的方式训练模型。分为n

   在实际比赛中往往不能单纯的使用某一种模型融合方法。
#### 06-07

##### 什么是长短时依赖long short term dependency.

在前馈网络或者循环网络中，当计算图变得极深的时候，神经网络优化算法所面临的一个难题就是**长时依赖**问题，—由于变深的神经网络结构使得模型丧失了学习先前信息的能力，让优化变得极其困难。因为循环神经网络在很长时间上的各个时刻循环重复应用相同操作，来构建很深的计算图，并且权重共享，这使得问题凸显。

在学习循环神经网络的过程中，长时依赖是面临的挑战，其根本原因是，经过很多阶段的传播后的梯度倾向于消失（大部分情况）或爆炸（极少，但对优化过程影响很大）。

http://xiaosheng.me/2017/06/26/article74/

https://blog.csdn.net/qq_28437273/article/details/79656605

##### 如何解决长时依赖问题 - 三种方法

> https://zhuanlan.zhihu.com/p/34490114
>
> https://blog.csdn.net/niaolianjiulin/article/details/79125677#t10

> 门机制、跨尺度连接和特殊初始化

本文是个人长期以来积累的一些模糊的想法，基本上不涉及具体公式，有些部分可能没有确凿的证据证实，仅仅是个人观点而已，抛出来供各位一起思考。

理论上 Simple RNN（或者叫 Vanilla RNN）就已经是图灵完备的了，能够模拟任何程序。但是实践中总是不尽如人意，因为存在性（存在一组模型设置能达到某个目的）和可学习性（真的通过训练能够找到这组参数设置）是两回事。

------

目前大约有三类机制解决长期依赖的学习问题，分别是**门机制、跨尺度连接和特殊初始化（及其维持）**。

###### **【门机制】**

- 代表作

- - LSTM: Long Short-Term Memory

  - GRU: Gated Recurrent Unit

  - Minimal Gated Unit，详见 [[1603.09420\] Minimal Gated Unit for Recurrent Neural Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.09420)

  - Simple Recurrent Unit，详见 [[1709.02755\] Training RNNs as Fast as CNNs](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1709.02755)

  - QRNN: [Quasi-Recurrent Neural Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.01576)

  - RAN: [Recurrent Additive Networks](https://link.zhihu.com/?target=http%3A//kentonl.com/pub/llz.2017.pdf)

  - - 这篇文章简化了一下 LSTM，扔掉了输出门，每一步的 candidate cell state 只考虑当前步的输入而不管历史信息。最后发现效果跟原来的 LSTM 差不多，参数却少了很多。于是认为非线性不那么重要，门机制才重要。
    - 这篇论文还把 GRU 表示成了一种奇怪的、类似于 LSTM 的形式，有利于比较 LSTM 和 GRU 的异同，并且认为 RAN 同时是 LSTM 和 GRU 的简化版。

- 解释

- - 这类 RNN Cell 大家应该很熟悉了，**其主要特点是用门控制信息流动，隐层状态采用加性更新，不做非线性变换**。具体的更新公式就不写了，下面只写一些理解。

  - 可以参考这篇文章 [Written Memories: Understanding, Deriving and Extending the LSTM](https://link.zhihu.com/?target=https%3A//r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)，以下内容也有很大一部分也源于这里。这里也简单导读一下这篇文章：

  - - **这篇文章讲了 LSTM 设计的初衷和原则、然后根据这些原则推导出了 GRU 的设计。**我先前一直觉得 LSTM 的门设计的很自然，而 GRU 是一种对 LSTM 的很奇怪的简化。而上面**这篇文章成功地说服了我 GRU 的设计很合理**，觉得 GRU 的门设计很奇怪的同学不要错过这篇文章。
    - 这篇文章认为 LSTM 里的读写顺序和状态分裂为 (c, h) 等设计很奇怪（文中叫 LSTM hiccup）。事实上，这也确实给初学者理解和编程实现带来了很多麻烦，例如 TF 中专门有一个类叫 LSTMStateTuple。不过，也可以从别的视角来解释和完善这种设计，参见 [从信息隐匿的角度谈 LSTM：从 Stack 到 Nest](https://zhuanlan.zhihu.com/p/34500721)。

  - 理论上，各个门的值应该在 [0, 1] 之间。但是如果你真正训过一些表现良好的网络并且查看过门的值，就会发现**很多时候门的值都是非常接近 0 或者 1 的**，而类似于 0.2/0.5 这样的中间值很少。从直觉上我们希望门是二元的，控制信息流动的通和断，事实上训练良好的门也确实能达到这种效果。

  - 加入门机制可以解决普通 RNN 的梯度消失的问题，网络上相关的文章很多，这里就不仔细推了。

  - 更重要的是，**门可以控制信息变形（information morphing）和选择性（selectivity）**。

  - - 这一点可以这么思考：LSTM 1997 年刚提出的时候没有 forget gate，各个步骤的 c 之间的导数就有一个完美的单位阵，号称 Constant Error Carousel。后来 2000 年 Gers 给它加入了 forget gate，假如梯度消失是网络难以训练的最重要的原因，那为什么加入 forget gate 有可能导致梯度被截断，反而模型效果更好了？

    - **选择性体现在，想让信息流动的时候的就让它流动，不想让它流动的时候就关掉。**例如做情感分析时，只让有情感极性的词和关联词等信息输入进网络，把别的忽略掉。这样一来，网络需要记忆的内容更少，自然也更容易记住。同样以 LSTM 为例，如果某个时刻 forget gate 是 0，虽然把网络的记忆清空了、回传的梯度也截断掉了，但这是 feature，不是 bug。

    - - 这里举一个需要选择性的任务：给定一个序列，前面的字符都是英文字符，最后以三个下划线结束（例如 "abcdefg___"）；要求模型每次读入一个字符，在读入英文字符时输出下划线，遇到下划线后输出它遇到的前三个字符（对上面的例子，输出应该是 "_______abc"）。显然，为了完成这个任务，模型需要学会记数（数到 3），只读入前三个英文字符，中间的字符都忽略掉，最终遇到 _ 时再输出它所记住的三个字符。“只读前三个字符”体现的就是选择性。

    - **信息变形体现在，模型状态在跨时间步时不存在非线性变换，而是加性的。**假如普通 RNN 的状态里存了某个信息，经过多个时间步以后多次非线性变换把信息变得面目全非了，即使这个信息模型仍然记得，但是也读取不出来它当时到底存的是什么了。而引入门机制的 RNN 单元在各个时间步是乘上一些 0/1 掩码再加新信息，没有非线性变换，这就导致网络想记住的内容（对应掩码为 1）过多个时间步记得还是很清楚。

###### **【跨尺度连接】**

- 代表作

- - CW-RNN: Clockwise RNN

  - - 大意是普通 RNN 都是隐层从前一个时间步连接到当前时间步。而 CW-RNN **把隐层分成很多组，每组有不同的循环周期**，有的周期是 1（和普通 RNN 一样），有的周期更长（例如从前两个时间步连接到当前时间步，不同周期的 cell 之间也有一些连接。这样一来，距离较远的某个依赖关系就可以通过周期较长的 cell 少数几次循环访问到，从而网络层数不太深，更容易学到。

  - Dilated RNN

  - - 和 CW-RNN 类似，只是 CW-RNN 是在同一个隐层内部分组，Dilated RNN 是在不同的层分组：最下面的隐层每个时间步都循环，较高的隐层循环周期更长些，从而有效感受野更大。

  - NARX RNN: Nonlinear Auto-Regressive eXogenous RNN

  - - 详见 [Learning Long-Term Dependencies in NARX Recurrent Neural Networks](https://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.1032.9079%26rep%3Drep1%26type%3Dpdf)
    - 类此以上两种方法，但前面的方法是把隐层单元分组，有的单元单步循环，有的多步循环；而这种方法是让每个隐层单元都在不同尺度上循环，例如某个隐层状态直接依赖于它的前一个、前两个、直到前 n-1 个隐层状态
    - 如果说普通的 RNN 是一阶递推式，这种就是 n 阶递推式

  - TKRNN: Temporal Kernel RNN

  - - 详见 [Temporal Kernel Recurrent Neural Networks](https://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%7Eilya/pubs/2008/tkrnn.pdf)
    - 类似 NARX RNN，只是把 n 阶递推式写成了特殊的、便于参数共享的形式，从而计算起来更快

- 解释

- - **既然学习长期依赖很难，那就手动把依赖的步数缩短，然后学习短期依赖就可以了。**思想有点儿类似于 ResNet 中的 skip-connection（但是跨越多个时间步的连接用的不是单位阵，而是需要学习的稠密矩阵），使得模型输出层可以看到之前不同时间步的信息，进而达到类似模型集成的效果。

###### **【特殊初始化（及其维持）】**

- 代表作

- - IRNN: Identity Recurrent Neural Networks

  - - 参考 [[1504.00941\] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1504.00941) 。
    - 大意就是普通的 RNN，把激活函数换成 ReLU，隐层的自循环矩阵用单位阵 ![I](https://www.zhihu.com/equation?tex=I) 初始化、偏置项设置成 0，然后在语言模型等任务上效果跟 LSTM 差不多。
    - 恒等初始化 + ReLU 的效果是让模型一开始先记住所有信息，并且能不变形地复制到下一时间步（暂时不考虑负元素被置为零的问题），之后模型再慢慢学习如何平衡长短距离的依赖信息。
    - LSTM 在 1997 年刚出来的时候没有遗忘门（相当于遗忘门恒为 1），其实也是这种设计思想
    - 同时作者也说了，如果该任务不需要长期依赖，把自循环矩阵初始化成 ![\alpha I (\alpha \in [0, 1))](https://www.zhihu.com/equation?tex=%5Calpha+I+%28%5Calpha+%5Cin+%5B0%2C+1%29%29) 更好，有助于模型快速忘掉长期信息。

  - np-RNN: normalized-positive definite RNN

  - - 详见 [[1511.03771\] Improving performance of recurrent neural network with relu nonlinearity](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.03771)
    - IRNN 的续作。IRNN 效果也不错，但是对超参数的选取较为敏感。这篇文章从 dynamic system 的角度分析了 ReLU RNN 的 fixed point （就是有些隐层状态会一直卡在同一个地方迭代后几乎不动）的演化行为，提出新的初始化方法，比 IRNN 更容易训练成功
    - 具体的做法是将 ReLU RNN 的自循环矩阵初始化成最大特征值为 1 的半正定矩阵。根据文章里的分析，这种设置可以尽量避免隐层状态退化到低维的 stable fixed point 上，从而有助于训练过程。

  - RIN: Recurrent Identity Network

  - - 详见 [[1801.06105\] Overcoming the vanishing gradient problem in plain recurrent networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.06105)
    - 类似于刚才讲的 IRNN，使用 ReLU 做激活函数，区别是把隐层的自循环矩阵参数化为 ![U+I](https://www.zhihu.com/equation?tex=U%2BI) ，其中 ![U](https://www.zhihu.com/equation?tex=U) 是需要学习的参数而 ![I](https://www.zhihu.com/equation?tex=I) 是单位阵。有点像残差学习的思路，学习的是自循环矩阵和单位阵相比差了多少。

  - Unitary-RNN

  - - 详见 [[1511.06464\] Unitary Evolution Recurrent Neural Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.06464)
    - 正交矩阵有个好处是特征值都是 1，连乘很多次也不会爆炸或者消失；很多框架默认的 RNN （包括 LSTM 等现代 RNN 单元）初始化方法都包含正交初始化，也是这一思路的体现；当然现在也有很多人直接使用 uniform distribution 初始化，不用正交初始化了。
    - Unitary-RNN 在正交化这一点上更是一条路走到黑，初始化时使用酉矩阵（正交矩阵的复数版本，它的权重参数是复数而不是实数），然后通过特殊的参数化技巧把权重矩阵表达成一个形式上较复杂但计算上很高效的形式，使得参数更新后也能保证新的参数矩阵仍然是酉矩阵，从而训练过程中一直都不会发生梯度消失或爆炸。
    - 其实 Unitary-RNN 这一系列复杂操作的目的仅仅是高效地维持矩阵的正交性。最简单的实现这一目的的方法是梯度投影法：先梯度下降，然后把参数矩阵投影成正交阵，只是这么做效率太低。
    - 文章里的实验在 copying memory problem 上效果拔群，远超 LSTM。不过这是一个刻意构造的 toy task 了。

  - IndRNN:

  - - 详见 [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.04831)
    - 大意是把自循环矩阵设置为对角阵，这样就把隐层的各个维度分离开了，导数的形式就从矩阵连乘变成了标量连乘，更容易分析
    - 但是普通的标量连乘还是会导致梯度消失/爆炸，所以 IndRNN 在训练的时候会对权重做裁剪（是 weight clipping，而不是 gradient clipping），把权重强行控制在某个范围，就可以使得它自乘若干次以后不至于太大或者太小
    - 吐个槽，这种做法感觉比 gradient clipping 还粗暴，居然还能奏效……

  - singular value clipping

  - - 详见 [Preventing Gradient Explosions in Gated Recurrent Units](https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/6647-preventing-gradient-explosions-in-gated-recurrent-units.pdf)
    - 大意是说 RNN 的梯度爆炸是由非线性系统的 bifurcation point 引起的。RNN 的 fixed point （即 RNN 运行一步后隐状态保持不变的点）有些是稳定的，有些是不稳定的，在 bifurcation point 附近梯度会暴增产生梯度爆炸。文章在某种特殊情形下找到了一个 stable fixed point，然后通过在训练过程中裁剪参数矩阵的 singular value，使得参数矩阵一直处于他找到的特殊情形下，从而参数永远在比较稳定的区域内更新，不会发生梯度爆炸，训练过程更稳定。
    - 注意，即便是 GRU/LSTM 等现代 RNN 单元也只能解决梯度消失，不能解决梯度爆炸。很多人都以为 GRU/LSTM 可以解决梯度爆炸问题，这是一个常见误解，详情请参考 [Why can RNNs with LSTM units also suffer from "exploding gradients"?](https://link.zhihu.com/?target=https%3A//stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients)），梯度爆炸现在主要靠 gradient clipping 来解决。
    - 最有趣的地方是，这篇文章和上面的 np-RNN 都基于对 dynamic system 和 fixed point 的分析，但是出发点却完全相反：np-RNN 想让模型离开 fixed point，而这篇文章想让模型别走出 fixed point 附近。我个人对于 dynamic system 不太熟悉，不过 fixed point 可能是 RNN 有时会出现连续重复预测现象的原因。到底应该避开 fixed point 还是应该利用它可能还需要进一步的研究。

  - SCRNN：Structurally Constrained Recurrent Neural Network

  - - 大意是把隐层分成两部分，其中一部分的更新方法是每次自乘 ![\alpha\in[0, 1)](https://www.zhihu.com/equation?tex=%5Calpha%5Cin%5B0%2C+1%29) 再加该步的新信息，被叫做 context feature；另一部分则类似于普通 RNN，根据前一时间步的值和当前时间步的 context feature 来更新。
    - 这里的 context feature 的更新也是加性的，不存在非线性变换。![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 的值接近 1（例如 0.95），近似于恒等映射，让保存的历史信息衰减得慢一些。
    - 这里将 RNN 的隐层分裂为两部分，其实也类似于 LSTM 中 (c, h) 分裂的操作。其中 context feature 类似于 LSTM 的 c，储层长期信息。

- 解释

- - **特殊初始化（及其维持）大致有三种方法：（近似）恒等映射、正交化、参数范围控制。**其中（近似）恒等映射（ ![I](https://www.zhihu.com/equation?tex=I) 或 ![\alpha I](https://www.zhihu.com/equation?tex=%5Calpha+I) ）以 IRNN 等为代表，可以被看成是一种简化版的遗忘门；正交化以 Unitary-RNN 为代表；参数范围控制以 IndRNN (weight clipping)/gradient clipping/singular value clipping 为代表。

  - - 其实参数范围控制在 RNN 以外的地方用的也挺多的。隐式地控制例如 L1/L2 正则，或者是 WGAN-GP 里的 gradient penalty；显式地裁剪例如 WGAN 中的 weight clipping，或者是 SN-GAN（[[1802.05957v1\] Spectral Normalization for Generative Adversarial Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.05957v1)） 里的 Spectral Normalization。

  - 对于（近似）恒等映射而言，这种方案首先让模型记住所有历史信息，然后在学习过程中慢慢恢复模型的表达能力，其实是**牺牲了模型短期内拟合复杂函数的能力**。设想，假如数据的分布是随着时间变化的，门机制可以更快地对新数据带来的改变做出适应，而这种方案可能就凉了。

  - 除了在 RNN 中，CNN 也可以利用类似的技巧。例如这篇文章（ [If resnets are the answer, then what is the question?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08591) ）里提出 LL-Init（looks linear initialization）的技巧，可以在不加 skip-connection 的情况下训练 200 层深的网络，办法就是先把网络初始化成一个线性函数，然后让它慢慢恢复自己的复杂结构的拟合能力。另外这篇论文对 ResNet 所解决的问题论证得很漂亮（很多人以为 ResNet 是解决梯度弥散问题，但并不是！因为 BatchNorm 就足够解决梯度弥散了，ResNet 解决的显然是另外的问题），值得一读。

------

此外，还有一些方法是上述三种思想结合的产物，例如：

- Statistical Recurrent Unit，详见 [[1703.00381\] The Statistical Recurrent Unit](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.00381)

- - 大意是认为模型知道所有历史信息的话对预测会有帮助，所以采用 moving average 对序列每一步得到的一些统计量做更新，然后模型以此为基础得到输出。使用的主要机制是近似恒等映射：论文中的公式 (7) 相当于把 LSTM 遗忘门固定成 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha)
  - 在我看来这个模型其实和 LSTM 差不多：SRU 中的 statistics (![\mu](https://www.zhihu.com/equation?tex=%5Cmu)) 对应 LSTM 的 cell state (![c](https://www.zhihu.com/equation?tex=c))， SRU 中的 ![\varphi](https://www.zhihu.com/equation?tex=%5Cvarphi) 对应 LSTM 的 new candidate state (![\tilde{c}](https://www.zhihu.com/equation?tex=%5Ctilde%7Bc%7D)) ，SRU 中的 ![o](https://www.zhihu.com/equation?tex=o) 对应 LSTM 的 ![h](https://www.zhihu.com/equation?tex=h)。但是作者讲故事的功底很好，不得不服= =
  - 由于 ![\alpha<1](https://www.zhihu.com/equation?tex=%5Calpha%3C1)，因此 ![\mu](https://www.zhihu.com/equation?tex=%5Cmu) 会随着时间指数衰减；而不同的 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 的值对应的衰减速度不同，使用多个不同的 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 就能使得输出 ![o](https://www.zhihu.com/equation?tex=o) 看到不同衰减速度的统计信息，这里相当于跨尺度连接

- Fourier Recurrent Unit，详见 [Learning Long Term Dependencies via Fourier Recurrent Units](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.06585.pdf)

- - 本文从上面的 Statistical Recurrent Unit 发展而来，改进了“遗忘门” ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 呈指数衰减的缺点。具体做法是将统计信息 ![u](https://www.zhihu.com/equation?tex=u) 的更新公式改为 ![u^{(t)} = u^{(t-1)} + \dfrac{1}{T} C^{(t)} \cdot h^{(t)}](https://www.zhihu.com/equation?tex=u%5E%7B%28t%29%7D+%3D+u%5E%7B%28t-1%29%7D+%2B+%5Cdfrac%7B1%7D%7BT%7D+C%5E%7B%28t%29%7D+%5Ccdot+h%5E%7B%28t%29%7D) ，其中 ![C^{(t)}](https://www.zhihu.com/equation?tex=C%5E%7B%28t%29%7D) 是一个包含不同频率余弦系数的矩阵。这里去掉了衰减因子 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) ，使用了恒等映射。
  - 最终的效果是，如果把 ![u](https://www.zhihu.com/equation?tex=u) 沿时间轴完全展开，可以发现不会再像 exponential moving average 那样重短期信息、轻长期信息，而是在不同余弦频率下起起伏伏，总有合适的频率能够保存到很多时间步之前的信息。这相当于是一个**动态权重的、无限长时间步的多尺度连接**，实在是厉害。
  - 文章证明了 FRU 的梯度是有上下界 bound 住的，梯度大小和时间步数无关，因此没有梯度消失和梯度爆炸问题，这一点非常厉害，是其他模型所不具备的。

------

三种方案当然各有优劣，其中门的好处自然是选择性，并且是现在比较流行的方案，对于各种任务效果都比较 robust。而后两种方案网络结构更简单，可能更适应于某些特殊的计算平台，或者是对于不太需要选择性的任务可能会有很好的效果，如以下回答中所描述的那样：

RNN中为什么要采用tanh而不是ReLu作为激活函数？[图标](https://zhstatic.zhihu.com/assets/zhihu/editor/zhihu-card-default.svg)

这个回答最后的总结 “『如果把ReLU RNN的参数增加四倍到跟LSTM的参数一样多，它应该是会稳定好过LSTM的』这个应该限定到我熟悉的语音识别任务，对NLP等等可能不大对。”其实说的也就是我在这里论述的选择性的问题：毕竟语音识别所需要的选择性不太强（和情感分析等相比）。

这里，特别要感谢 

***方法总结：跳跃连接、渗透单元（设置自连接单元从而获得导数近似1。）、删除连接、门控LSTM、门控GRU、梯度截断、信息流中的正则化（解决梯度消失和捕获长期依赖问题的想法是梯度乘积尽量为1，lstm和gru门控机制是一种方式。另一种方法就是使梯度向量在反向传播过程中尽量维持幅度。如下：）***

---

##### Attention机制的原理及用途

> https://zhuanlan.zhihu.com/p/31547842

1. Attention 机制解决的问题

   1. 在《sequence to sequence learning with neural network》中介绍了一种基于RNN的seq2seq模型，基于一个encoder和一个decoder来构建神经网络的end-to-end机器翻译模型，其中encoder将输入X编码为一个固定长度的隐变量Z，decoder基于隐变量Z解码出目标输出Y。
   2. 但其中存在两个问题：
      1. 把输入X的所有信息压缩到一个固定长度的隐变量Z，忽略了输入X的长度，当输入的句子长度很长时候，模型的性能会急剧下降。
      2. 当把输入X编码为一个固定长度的时候，对于句子中的每个词赋于相同的权重是不合理的。在翻译中有的词明显要比别的词更重要。

2. Attention 原理

   1. 在Seq2Seq模型中，想要解决的主要问题是，如何在机器翻译中，把变长的输入X映射到一个变长输出Y的问题。
   2. ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/atten.jpg)
   3. **通过加入注意力机制，在翻译当前节点j的时候，通过对查看encoder端的各个位置对当前节点j的影响，从而对decoder端加入了新的参数：即各个encoder端隐层状态的和。而计算对节点j的影响的方式各种各样，**
   4. ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/atten1.jpg)
   5. 其中,Score(ht,hs) = aij表示源端与目标单单词对齐程度。可见，常见的对齐关系计算方式有，点乘（Dot product），权值网络映射（General）和concat映射几种方式。

3. Attention 的分类

   1. soft Attention 和 hard Attention
      1. 上面提到的就是soft Attention。是参数化的，因此可导，可以被嵌入到模型中去，直接训练。梯度可以经过Attention Mechanism模块反向传播到其他部分。
      2. Hard Attention是一个随机的过程，hard attention不会选择整个encoder的输出作为其输入。随机选择一部分进行计算。所以无法进行反向传播。
   2. Global Attention && Local Attention
      1. Global Attention就是一开始说的，会考虑所有的因层状态用于计算context vector 的权重。
      2. Local Attention会选取encoder端的一部分因层状态进行计算。
   3. Self Attention
      1. 与传统的Attention机制非常的不同，传统的Attentiuon是基于source端和target端的隐变量计算Attention的，得到的结果是远端的每个词与目标端的每个词之间的依赖关系。但self attention不同，他分别在source端和target端进行，仅与source端和target端自身相关的self attention，捕捉source端或者targe端自身的词与词之间的依赖关系；
      2. 源自于《attention is all you need》

4. Attention的其他一些应用

   1. Hierachical Attention
      1. ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/atten2.jpg)
   2. Attention over attention
      1. ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/atten3.jpg)

5. 应用

   1. 机器翻译

      > 给定一个英文句子作为输入序列，翻译并输出一个中文句子作为输出序列。Attention用于关联输出序列中每个单词与输入序列中的某个特定单词的关联程度。

   2. 图像标注

      > 基于序列的Attention Mechanism可以应用于计算机视觉问题， 以帮助理解如何最好的利用卷机神经网络来生成一段关于图片的描述，也称为caption

   3. 蕴含关系推理

      > 给定一个用英语描述前景描述和假设，判读假设与假设的关系：矛盾，相关或者包含。

   4. 语音识别

   5. 文字摘要生成

      > 给定一篇英文文章作为输入，输出一个总结英文文章内容的摘要的句子。
      >
      > Attention用于将输出摘要中的每个单词与输入文档中的特定单词相关联。

##### SQuAD文本理解挑战赛十大模型解读

> https://mp.weixin.qq.com/s/KVONdK44T8JKU5pFB5ydrQ
>
> 在斯坦福大学发起的 SQuAD（Stanford Question Answering Dataset）文本理解挑战赛中，微软亚洲研究院和阿里巴巴的 R-NET 模型和 SLQA 模型在 EM 值（表示预测答案和真实答案完全匹配）上分别以 82.650 和 82.440 的成绩率先超过人类（82.304）。

教机器学会阅读是近期自然语言处理领域的研究热门之一，也是人工智能在处理和理解人类语言进程汇总的一个长期目标。得益于深度学习技术和大规模标注数据集的发展，用端到端的神经网络来解决阅读理解任务取得了长足的进步。

##### 序列标注：BiLSTM-CRF模型做基于字的中文命名实体识别

> https://www.cnblogs.com/Determined22/p/7238342.html
>
> [序列标注任务的常见套路](https://zhuanlan.zhihu.com/p/35620631)

NER的方法笼统的分为三类方法：

1. 基于规则的方法：利用手工编写的规则，将文本与规则进行匹配来识别出命名实体。

2. 基于特征模版的方法：统计机器学习方法将NER视为是序列标注问题，从大规模语料来学习出标注模型，从而对句子中的各个位置进行标注。

3. 基于神经网络的方法：

   随着硬件能力和词的分布式表示的出现，神经网络成为可以处理许多NLP任务的模型。这类方法对于序列标注如CWS、POS、NER的处理方式是类似的，将token从离散的one-hot表示映射到低纬度的embedding中，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法是的模型的训练成为一个端到端的整体过程，而非传统的pipeline，不依赖特征工程，是一种数据驱动的方法；但网络变种多、对参数设置依赖大，模型可结实性差。

   但是上述模型的缺点是，单独预测每个token，无法使用上下文，于是有了LSTM+CRF来做句子级别的标签预测。

   ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/bilstm-crf.png)

##### Dropout





#### end

