---
layout: post
title: "04.23到04.29.home"
author: "Spurs"
date: 2018-04-29 12:40:00
description:
photos:
links:
categories:
tags:
---

> 04.23到04.29.home

<!-- more -->

### 04-22

#### 二分类性能评价指标

> [链接](http://www.cnblogs.com/zhaokui/p/ml-metric.html)

1. 精确率和召回率

   精确率precision指的是模型判为正的所有样本中有多少是真正的正样本。 - 查准率

   召回率recall指的是所有正样本有多少被模型判为了正样本。 - 查全率

   为了再precision和recall之间权衡，

   有一种选择是准确率-召回率曲线(P-R curve)，曲线下面积为AP分数（average precision score）

   ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/P_R_cure.png)

   另一种选择是\\( $F_{\beta}$ \\)分数：

   ​	$$F_{\beta}=(1+\beta^2)\cdot\frac{\text{precision}\cdot\text{recall}}{\beta^2\cdot\text{precision}+\text{recall}} $$

   当\\( $\beta$ = 1 \\)时候，称为F1分数，是分类和信息检索中最常用的指标之一

2. [ROC曲线和AUC值](https://blog.csdn.net/dazhi_100/article/details/73469366)

   ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/auc.png)

   ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/auc_1.png)

   AUC分数是曲线下的面积（Area under curve），越大意味着分类器效果越好。

   ROC曲线适用于二分类问题，以假正率为横坐标，真正率为纵坐标的曲线图

   - 基本理解

     **TP**:正确的肯定数目

     **FN**:漏报，没有找到正确匹配的数目

     **FP**:误报，没有的匹配不正确

     **TN**:正确拒绝的非匹配数目

   - **AUC意味着什么**

     那么AUC值的含义是什么呢？根据(Fawcett, 2006)，AUC的值的含义是：

     The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.

     这句话有些绕，我尝试解释一下：首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。

   - **为什么使用ROC曲线**
     既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线5的对比：

   - sklearn中的计算方法

     ```python
         check_consistent_length(x, y)
         x = column_or_1d(x)
         y = column_or_1d(y)

         if x.shape[0] < 2:
             raise ValueError('At least 2 points are needed to compute'
                              ' area under curve, but x.shape = %s' % x.shape)

         direction = 1
         if reorder:
             # reorder the data points according to the x axis and using y to
             # break ties
             order = np.lexsort((y, x))
             x, y = x[order], y[order]
         else:
             dx = np.diff(x)
             if np.any(dx < 0):
                 if np.all(dx <= 0):
                     direction = -1
                 else:
                     raise ValueError("Reordering is not turned on, and "
                                      "the x array is not increasing: %s" % x)

         area = direction * np.trapz(y, x)
         if isinstance(area, np.memmap):
             # Reductions such as .sum used internally in np.trapz do not return a
             # scalar by default for numpy.memmap instances contrary to
             # regular numpy.ndarray instances.
             area = area.dtype.type(area)
         return area
     ```

   > 参考：
   >
   > [ROC和AUC介绍以及如何计算AUC](http://alexkong.net/2013/06/introduction-to-auc-and-roc/)
   >
   > [机器学习性能评估指标](http://charleshm.github.io/2016/03/Model-Performance/#)
   >
   > ​
   >
   > [AUC计算方法总结](https://blog.csdn.net/dazhi_100/article/details/73469366)
   >
   > [模型评价(一) AUC大法](https://segmentfault.com/a/1190000010410634)
   >
   > [精确率与召回率，RoC曲线与PR曲线](http://www.cnblogs.com/pinard/p/5993450.html)

3. 对数损失

   > 亦称逻辑回归损失，交叉熵损失

   对于二分类问题

   ​	$$L_{\rm log}(y,p)=-\log{\rm Pr}(y|p)=-(y\log(p)+(1-y)\log(1-p))$$

   对于多分类问题

   ​	$$L_{\log}(Y_i,P_i)=-\log{\rm Pr}(Y_i|P_i)=\sum\limits_{k=1}^{K}y_{i,k}\log p_{i,k}$$

4. Hinge loss, 铰链损失，边缘最大化

   铰链损失最开始出现在二分类问题中，假设正样本被标记为1，负样本被标记为-1，yy是真实值，ww是预测值，则铰链损失定义为：

   $$L_{\text{Hinge}}(w, y)=\max\{1-wy,0\}=|1-wy|_+$$

   然后被扩展到多分类问题，假设ywyw是对真实分类的预测值，ytyt是对非真实分类预测中的最大值，则铰链损失定义为：

   $$L_{\text{Hinge}}(y_w, y_t)=\max\{1+y_t-y_w,0\}$$

   注意，二分类情况下的定义并不是多分类情况下定义的特例。

5. 平均绝对误差MAE，L1范数损失

   $${\rm MAE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}|y_i-\hat{y}_i|$$

6. 平均平方误差MSE，l2范数损失

   $${\rm MSE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}(y_i-\hat{y}_i)^2$$

#### 好的博客，适合每天阅读的那种

1. https://swe.mirsking.com/
2. https://plushunter.github.io/
3. http://www.csuldw.com/
4. http://alexkong.net/index.html
5. http://charleshm.github.io/
6. http://www.cnblogs.com/pinard/

#### 判别模型和生成模型

1. https://blog.csdn.net/zouxy09/article/details/8195017
2. https://blog.csdn.net/qq_34896915/article/details/73771747

#### 数组随机打乱顺序

- 相当于从袋子里取球，每次随机的取一个球。
- [地址](https://www.zhihu.com/question/68330851/answer/266506621)

### 04-23

#### Leetcode 中的问题解题思路

1. 想清楚问题，理解清楚，至少花五分钟

2. 先处理特殊情况

3. ```c++
   istringstream in(str);
   // 可以有效了处理字符串split问题
   ```

#### 机器学习中概率分布

1. https://ayase.moe/2017/05/28/common-dist-in-ml/

2. 指数分布

   ```c++
   f(x)=λe**(-λx), x>0
   f(x)=0, x<=0

   其中 λ>0,叫做指数分布, X~E(λ)

   分布函数
   F(x) = 1-e**(-λx), x>0
   F(x) = 0, x<=0

   当一个时间发生了，那么，在此基础上在发生一个delta事件的概率是独立的。
   例如:
   交通事故发生的时间间隔服从指数分布，那么，过去的13个小时没有发生事故，再经过两个小时发生事故的概率是多少？
   无论前面是否发生事故，经过两个小时发生的事故的概率都是一样大的。
   P(X>13+2|X>13)=P(X>2)=1-F(2)
   ```

3. 时间独立性

   P(AB) = P(A)P(B)称作A和B是相互独立的事件。

   ```
       (1) P(AB) = P(A)P(B)
       (2) P(AC) = P(A)P(C)
       (3) P(BC) = P(B)P(C)
       (4) P(ABC) = P(A)P(B)P(C)

   其中: (1) (2) (3)成立，称作A B C两两独立；而 (1) (2) (3) (4)成立,都成立才称作相互独立。

   P(A∪B) = P(A) + P(B) - P(AB) = P(A) + P(B) - P(A)P(B)
   ```

4. 协方差

   计算公式: Cov(X,Y)=E(XY)-E(X)E(Y)

   ρ的理解很关键，表示两个随机变量的相关性。如果二者线性相关，那么使用1个随机变量就可以了，没有必要使用两个随机变量。

5. $$D(x) = E(x^2) - [E(x)]^2$$

| 分布名称             | 期望             | 方差               |
| ---------------- | -------------- | ---------------- |
| 泊松分布 X~π(λ)      | E(X)=λ         | D(X)=λ           |
| 正态分布 X~N(μ,σ**2) | E(X) = μ       | D(X)=σ**2        |
| 指数分布 X~E(λ)      | E(X) = 1/λ     | D(X)=1/λ**2      |
| 二项分布 X~B(n,p)    | E(X) = np      | D(x)=np(1-p)     |
| 几何分布 X~G(p)      | E(X) = 1/p     |                  |
| 均匀分布 X~U(a,b)    | E(X) = (a+b)/2 | D(X)=(b-a)**2/12 |

6. http://www.csuldw.com/2016/08/19/2016-08-19-probability-distributions/#more


### 04-24

#### KNN

1. KNN中，当训练集、距离度量、k、分类决策规则确定之后，对于任何一个新的输入实例，他所属的类唯一确定。

2. 距离度量有：欧氏距离、曼哈顿距离、切比雪夫距离、闵可夫斯基距离、标准化欧式距离、余弦夹角

3. K值较小时候，就相当于用较小的领域中的训练实例进行预测，K值减小意味着模型变复杂了，容易发生过拟合

   K值较大时候，减少了学习的估计误差。

4. 分类决策规则：主要是多数表决

5. 简单、易于理解、易于实现、无需训练、无需估计参数、

6. 懒惰学习，只有当需要分类测试样例时再进行，lazy learner

   积极学习，一旦训练集可用，就开始学习从输入属性到类标号的映射模型。

   > https://plushunter.github.io/2017/01/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AK%E8%BF%91%E9%82%BB/

####  线性回归

1. http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html
2. https://plushunter.github.io/2017/01/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/
3. 最小二乘法或者梯度下降法或者牛顿法（海森矩阵）

#### 逻辑回归

1. https://plushunter.github.io/2017/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92/
2. 推导过程，从极大似然估计，到最小对数似然估计

#### 决策树

1. https://plushunter.github.io/2017/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/

#### 随机森林

1. https://plushunter.github.io/2017/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/

#### Adaboost

1. https://plushunter.github.io/2017/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost/




### 04-25

#### #include <cassert>, <algorithm>, 

[link_cassert](http://www.cplusplus.com/reference/cassert/assert/) [link_algorithm](http://www.cplusplus.com/reference/algorithm/)

```c++
#include <cassert>
assert(int expression)
```

#### string toupper(), tolower(), strupr(), strlwr()

```c++
#include<iostream>  
#include<string>  
  
using namespace std;  
  
int main(int argc, char* argv[])  
{  
    //声明字符数组  
    char str[80],*p;  
    int i;  
  
    //转换字符串中的小写为大写  
    cout<<"将字符串中的小写字母转换为大写"<<endl;  
    cout<<"请输入原字符串："<<endl;  
    cin>>str;  
    p=strupr(str);  
    cout<<"p:"<<p<<endl;  
    cout<<"string:"<<str<<endl;  
    cout<<"___________________"<<endl;  
  
    //转换字符串中的大写为小写  
    cout<<"将字符串中的大写字母转换为小写"<<endl;  
    cout<<"请输入原字符串："<<endl;  
    cin>>str;  
    p=strlwr(str);  
    cout<<"p:"<<p<<endl;  
    cout<<"string:"<<str<<endl;  
    cout<<"___________________"<<endl;  
  
    system("pause");  
    return 0;  
}  
```

```c++
#include <iostream>  
  
using namespace std;  
  
int main()  
{  
    cout<<(char)toupper(97)<<'\n';  
    cout<<(char)toupper('a')<<'\n';  
    cout<<(char)tolower(66)<<'\n';  
    cout<<(char)tolower('B')<<'\n';  
  
    return 0;  
}  
```

#### ubuntu 16 释放显卡内存

```sudo kill -9 PID```

###  04-26

#### RNN && LSTM && GRU

-  [深度学习系列（4）：循环神经网络（RNN）](https://plushunter.github.io/2017/04/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89/)

-  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm_04-26.png)

   - $$o_t=g(Vs_t)\\s_t=f(Ux_t+Ws_{t-1})$$

   - $$o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1}))$$

     $$=g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2 })))$$

     $$=g(Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3 }))))$$

   - $$\left[\begin{array}{c}    s_{1}^{t}\\    s_{2}^{t}\\    ·\\    s_{n}^{t}\\\end{array}\right]=f\left(\left[\begin{matrix}    u_{11}&        u_{12}&        ··&        u_{1m}\\    u_{21}&        u_{22}&        ··&        u_{2m}\\    ·&        ·&        ·&        ·\\    u_{n1}&        u_{n2}&        ···&        u_{nm}\\\end{matrix}\right]\left[\begin{array}{c}    x_1\\    x_2\\    ···\\    x_m\\\end{array}\right]+\left[\begin{matrix}    w_{11}&        w_{12}&        ···&        w_{1n}\\    w_{21}&        w_{22}&        ···&        w_{2n}\\    ··&        ··&        ···&        ··\\    w_{n1}&        w_{n2}&        ···&        w_{nn}\\\end{matrix}\right]\left[\begin{array}{c}    s_{1}^{t-1}\\    s_{2}^{t-1}\\    ···\\    s_{n}^{t-1}\\\end{array}\right]\right)$$

     ​


- [深度学习系列（5）：长短时记忆网络（LSTM）](https://plushunter.github.io/2017/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89/)

  > LSTM用两个门来控制单元状态Cell的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态$c_{t−1}$有多少保留到当前时刻$c_t$；另一个是输入门（input gate），他决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$。LSTM用输出门（output gate）来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-1.png)

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/rnn-04-26.png)

  - 遗忘门
    $$f_t=\sigma(W_f·[h_{t-1,x_t}]+b_f)$$

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-2.png)

  - 输入门

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-3.png)

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-4.png)

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-5.png)

  - 输出门

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-6.png)

    ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lstm-04-26-7.png)

- GRU是众多LSTM变体中最成功的一个

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/gru-04-26-0.png)


- [深度学习系列（6）：递归神经网络](https://plushunter.github.io/2017/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9A%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)


#### 神经网络：防止过拟合

> 无非是从数据和模型两个角度去分析解决问题

[深度学习系列（11）：神经网络防止过拟合的方法](https://plushunter.github.io/2017/05/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95/)

- 所有的过拟合无非就是训练样本的缺乏和训练参数的增加。一般要想获得更好的模型，需要大量的训练参数，这也是为什么CNN网络越来越深的原因之一，而如果训练样本缺乏多样性，那再多的训练参数也毫无意义，因为这造成了过拟合，训练的模型泛化能力相应也会很差。大量数据带来的特征多样性有助于充分利用所有的训练参数。
- 有时候往往拥有更多的数据胜过一个好的模型

#### 鞍点

**Hinton提出高维优化问题中根本没有那么多局部极值。高维非凸优化问题之所以困难，是因为存在大量的鞍点（梯度为零并且Hessian矩阵特征值有正有负）而不是局部极值**

**和局部极小值相同的是，在该点处的梯度为零，不同在于在鞍点附近Hessian矩阵特征值有正有负，即不定的，而在局部极值附近的Heesian矩阵是正定的。**

**再鞍点附近，基于梯度的优化方法（几乎目前所有的实际使用的优化算法都是基于梯度的）会遇到较为严重的问题，按点附近梯度很小，可能会长时间卡在该点附近。在鞍点数目极大的时候，这个问题会变得很严重。**

#### [深度学习系列（10）：卷积神经网络分享](https://plushunter.github.io/2017/05/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E4%BA%AB/)

> 非常值得读一下

#### [深度学习系列（9）：Batch Normalization](https://plushunter.github.io/2017/05/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9ABatch%20Normalization/)

> batch normalization(Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的创新之一。实际上它并不是一个优化算法，而是一个自适应的重新参数化 的方法，试图解决训练非常深层模型的困难。
>
> **机器学习领域有一个很重要的假设：iid独立同分布假设 - *独立同分布*independent and identically distributed (*i.i.d.*) ，就是假设训练数据和测试数据满足相同分布，这是通过训练数据训练出来的模型能够在测试集上获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。**

- 但是这里有个问题，如果都通过Batch Normalization，那么不就跟把非线性函数替换成线性函数效果相同了？我们知道，如果是多层的线性函数变换，其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。

- 比如下图，在使用sigmoid激活函数的时候，如果把数据限制到零均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型的表达能力

- BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者由移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动，让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。从而保证整个网络的capacity。

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/bn-04-26-0.png)

- 论文中罗列了Batch Normalization的很多作用，一一列举如下：

  - 1）可以使用很高的学习率。如果每层的scale不一致，实际上每层需要的学习率是不一样的，同一层不同维度的scale往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization
  - 2）移除或使用较低的dropout。dropout是常用的防止overfitting的方法，而导致overfitting的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfitting现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%~50%相比，可以大大提高训练速度。
  - 3） 降低L2权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，现在用了Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。
  - 4）取消Local Response Normalization层。 由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。
  - 5）减少图像扭曲的使用。 由于现在训练epoch数降低，所以要对输入数据少做一些扭曲，让神经网络多看看真实的数据。

  说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。

#### [深度学习系列（8）：激活函数](https://plushunter.github.io/2017/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/)

> 建议用ReLU非线性函数。但是要注意初始化和learning rate的设置，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。

#### [深度学习系列（7）：神经网络的优化方法](https://plushunter.github.io/2017/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/)

下面两幅动画让我们直观感受一些优化算法的优化过程。

- 在第一幅动图中，我们看到他们随着时间推移在损失表面的轮廓（contours of a loss surface）的移动。注意到Adagrad、Adadelta和RMSprop几乎立刻转向正确的方向并快速收敛，但是Momentum和NAG被引导偏离了轨道。这让我们感觉就像看滚下山的小球。然而，由于NAG拥有通过远眺所提高的警惕，它能够修正他的轨迹并转向极小值。

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/opt1.gif)

- 第二幅动图中为各种算法在saddle point（鞍点）上的表现。所谓saddle point也就是某个维度是positive slope，其他维度为negative lope。前文中我们已经提及了它给SGD所带来的困难。注意到SGD、Momentum和NAG很难打破对称，虽然后两者最后还是逃离了saddle point。然而Adagrad, RMSprop, and Adadelta迅速地沿着negative slope下滑。

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/opt2.gif)

- 七、二阶方法

  在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下：

  x←x−[Hf(x)]−1∇f(x)x←x−[Hf(x)]−1∇f(x)

  这里Hf(x)是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。∇f(x)是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。

  然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，L-BFGS使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。

  然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。

  实践时在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。


#### [深度学习系列（1）：神经网络与反向传导算法](https://plushunter.github.io/2017/04/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95/)

> 真的很好的博客

#### [很好的技术博客plushunter](https://plushunter.github.io/tech-stack/)

#### 在买的书中，讲了并行化tensorflow以及分布式tensorflow。

#### 办公软件不止office，还有xmind思维导图

