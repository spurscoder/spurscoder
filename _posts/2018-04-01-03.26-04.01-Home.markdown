---
layout: post
title: "03.26到04.01.home"
author: "Spurs"
date: 2018-04-01 12:40:00
tags:
---

> 03.26到04.01.home

<!-- more -->

## 03-26

## 03-27

## 03-28

### 常见的损失函数

- 模型$f(x)$关于训练数据集的平均损失称为经验风险
- 经验风险最小化的策略认为，经验风险最小的模型是最优的模型，则按照经验风险最小化求得最优模型
- 当样本容量很小时候，经验风险最小化的策略容易产生过拟合的现象，结构风险最小化可以防止过拟合。
- 结构风险是在经验风险的基础上加上表示模型复杂度的正则化项。
- **这样，监督学习问题就变成了经验风险或结构风险最优化的问题。**

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lost1.jpg)

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/lost2.jpg)

### 简单介绍以下逻辑回归

- Logistic回归的目的是从特征学习出一个0/1分类模型，而这个模型是将特征的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数将自变量映射到（0,1）上，映射后的值被认为是属于y=1的概率。

### hashmap和hashtable的区别

- HashMap基于HashTable实现，不同之处在于HashMap是飞同步的，并且允许null，即null key和null value，HashTable则不允许null
- 凡是hashmap、hashset等带有hash字眼的都是基于hashtable实现的，
- 没带hash字眼的set、map等都是基于红黑树实现的，
- 前者无序，后者有序。
- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/hash.jpg)

### 分类问题中，经常遇到正负样本数据量不均衡的问题，有什么[处理方法](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)

> 例如正样本10w条数据，负样本1w条数据。

- 将负样本重复10次，生成10w样本量，打乱顺序参与分类
- 直接进行分类，可以最大限度利用数据
- 从10w正样本中，随机抽取1w参与分类
- 将负样本每个权重设置为10，正样本权重为1，参与训练过程。

> 以上几种方法各有优缺点，需要具体问题具体分析。

### 朴素贝叶斯分类的基本假设是每个变量相互独立

### 关于SVM支持向量机

- L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力。
- Hinge损失函数，作用是最小化经验分类错误
- 分类间隔是$2/\|w\|$，其中$\|w\|$代表向量的模

### 在HMM中

- EM算法，只有观测序列，无状态序列是学习模型参数，即Baum-Welch算法
- 维特比算法，用动态规划解决HMM的预测问题
- 前向后向算法，用来算概率
- 极大似然估计，即观测序列和状态序列都存在时候的监督学习算法，用来估计参数

### kmeans的复杂度？

```python
选择K的点作为初试质心
repeat
	将每个点指派到最近的质心，形成K的族
    重新计算每个族的质心
until 族不发生变化或达到最大迭代次数
```

- 时间复杂度：$O(tkmn)$，其中t为迭代次数，k为族的个数，m为记录数，n为维数
- 空间复杂度：$O((m + k) * n)$，其中k为族的个数，m为记录数，n为维数

### 研究网络的话，看到stride为1的时候，当kernel=3，padding=1或者kernel=5，padding=2，一看就是卷积前后尺寸不变。

### 聚类算法结果的影响因素有分类准则、特征选取、模型相似度测量

### Ensemble是论文刷结果的终极核武器，深度学习中一般有以下几种方式

- 同样的参数，不同的初始化方式
- 不同的参数，通过cross-validation，选取最好的几组
- 同样的参数，模型训练的不同阶段，即不同迭代次数的模型
- 不同的模型，进行线性融合，例如rnn和传统模型

### 什么是RNN

- RNNs的目的是处理序列数据。再传统的神经网络模型中，是从输入层到隐藏层再到输出层，层与层之间是全连接的，而每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能为力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNNs之所以称为循环神经网络，即一个序列的当前的输出与前面的输出有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前的输出的计算中，即**隐藏层之间的节点不再无连接而是有链接的，并且隐藏层的输入不仅包括输入层的输出还包括了上一时刻的隐藏层的输出**。

- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/rnn1.jpg)

- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/rnn2.jpg)

- 你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。**在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连**。 

- [完全图解RNN、RNN变体、Seq2Seq、Attention机制](https://zhuanlan.zhihu.com/p/28054589)

  > 文中介绍了经典的RNN和RNN的一些变化，例如（1 VS N，N VS 1，N VS N，N VS M）等结构变化，
  >
  > 其中的N vs M，这种结构又叫做Encoder-Decoder模型，也可以称之为Seq2Seq模型。
  >
  > 可以用于机器翻译，文本摘要，阅读理解，语音识别等。
  >
  > 对于长距离的依赖造成了模型性能的瓶颈，由此引入了Attention机制

- [循环神经网络(RNN, Recurrent Neural Networks)介绍](https://blog.csdn.net/heyongluoyao8/article/details/48636251)

- [Understanding LSTM Networks](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

- [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)


### 循环神经网络（recurrent）和递归神经网络（recurcive）

- RNN从时间和空间两个不同维度来看，主要有两种结构：Recurrent Neural Network(循环神经网络)和RecursiveNeural Network(递归神经网络)。主要对于前者进行学习。
- 循环神经网络是一个在时间上传递的神经网络，网络的深度就是时间的长度。该神经网络是专门用来处理时间序列问题的，能够提取时间序列的信息。如果是前向神经网络，每一层的神经元信号只能够向下一层传播，样本的处理在时刻上是独立的。对于循环神经网络而言，神经元在这个时刻的输出可以直接影响下一个时间点的输入，因此该神经网络能够处理时间序列方面的问题。
- RNN的重要特性是可以处理不定长的输入，得到一定的输出。将输入的序列使用RNN映射为一个固定大小的向量，然后将这个向量输入softmax层，用于分类和其他任务。随着，信息关联度越来越长，RNN 将变得无法去学习这些信息之间的联系，从而完全失去作用，为了解决RNN遗忘性这个问题，所以提出了 LSTM 的结构。同时对softmax和LSTM这两个概念进行学习。

### RNN中为什么要采用tanh而不是Relu作为激活函数？

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/rnn21.png)

那为什么同样的方法在RNN中不奏效呢？其实这一点Hinton在它的IRNN论文里面（arxiv：[[1504.00941\] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1504.00941)）是很明确的提到的：

![](https://pic1.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_hd.jpg)

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/rnn22.png)

### RNN中的[梯度问题](https://blog.csdn.net/han_xiaoyang/article/details/51932536)

- 梯度爆炸，在实验过程中，一旦梯度值增加很大，就会很容易探测其引起的溢出，这就是梯度爆炸问题

  > 解决方法：
  >
  > 截断模型

- 梯度弥散，当梯度值接近于零时，将会导致梯度的不断衰减，这就是梯度弥散的问题

  > 解决方法：
  >
  > 1. 将随机化W改为一个有关联的矩阵初始化
  > 2. 使用Relu代替sigmoid函数。

### LSTM和GRU是从RNN演变而来

### standford的[nlp课程](http://web.stanford.edu/class/cs224n/archive/WWW_1617/index.html)

### [迁移学习与fine-tuning有什么区别？](https://www.zhihu.com/question/49534423)

### 好文章-[用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践](https://zhuanlan.zhihu.com/p/25928551)

### [[译] 理解 LSTM 网络](https://www.jianshu.com/p/9dc9f41f0b29/)

### [详解梯度下降等各类优化算法](https://ask.julyedu.com/question/7913)

### [Adaboost算法的原理与推导](http://blog.csdn.net/v_july_v/article/details/40718799)

## 03-29

### 什么样的资料集不适合深度学习？

- 数据集太小，数据样本不足时，深度学习相对于其他机器学习算法，没有明显优势
- 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像、语音、自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理，
- 举个例子：预测一个人的健康状况，相关的特征会有年龄、职业、收入、家庭状况等因素，将这些因素打乱，并不会影响相关的结果。

### 当深度学习性能遇到瓶颈的时候，如何优化？

- [机器学习性能改善备忘录](https://blog.csdn.net/han_xiaoyang/article/details/53453145)
- 可以尝试基于数据、基于算法、用算法调参、借助模型融合等。

### 如何提高深度学习性能？

- [如何提高深度学习性能](https://blog.csdn.net/han_xiaoyang/article/details/52654879)

### 广义线性模型是怎样被应用于深度学习中？

- 从统计学的角度看深度学习，可以看做是递归的广义线性模型
- 广义线性模型相对于经典的线性模型$y=wx+b$，核心在于引入了**连接函数**，$g(.)$，形式变为了$y=g^{-1}(wx+b)$.
- 深度学习时递归的广义线性模型，神经元的激活函数，就是广义线性模型的连接函数。逻辑回归的Logistic函数即为神经元激活函数中的sigmoid函数，很多类似的方法再统计学和神经网络中有不同的名称，

### 机器学习面试中应该了解那些理论知识？

> 来源https://www.zhihu.com/question/62482926

1. 理论功底，主要考察对机器学习模型的理解，选择性提问(如果遇到面试者的研究方向自己不了解但感兴趣，会很欣喜，可以趁机学习一下)这块儿的问题会比价零碎，都是我自己深入思考过的**(背书是没有用的，这里任何一个点都可以给你展开问下去)**
   - 过拟合欠拟合（举几个例子让你判断下，顺便问问交叉验证的目的，超参数搜索方法，earlystopping）、L1和L2正则的做法、正则化背后的思想（顺便问问BatchNorm、Covariance Shift）、L1正则产生稀疏解的原理、逻辑回归为何线性模型（顺便问问LR如何解决低维不可分、从图模型的角度看LR和朴素贝叶斯和无监督）、几种参数估计方法、MLE/MAP/贝叶斯的联系和区别，简单说下SVM的支持向量（顺便问问KKT条件、为何对偶、核的通俗理解）、GBDT随机森林是否并行（顺便问问bagging boosting）、生成模型判别模型举个例子、聚类方法的掌握（顺便问问Kmeans的EM推导思路、谱聚类和Graph-cut的理解）、梯度下降方法和牛顿类方法的区别（顺便问问Adam、L-BFGS的思路）、本监督的思想（顺便问问一些特定半监督算法是如何利用无标签数据的、从MAP角度看半监督）、常见的分类模型的评价指标（顺便问问交叉熵、ROC如何绘制、AUC的物理含义、类别不均衡样本）
   - CNN中的卷积操作和卷积核的作用，maxpooling的作用、卷积层和全连接层的联系、梯度爆炸和消失的概念（顺便问问神经网络权重初始化的方法、为何能缓解梯度爆炸消失、CNN中有那些解决办法、LSTM如何解决的、如何梯度修剪、dropout如何再用在RNN系列网络中、dropout防止过拟合）、为何卷积可以用在图像、语音、语句上(顺便问问channel再不同类型数据源中的含义)
   - 如果面试者跟我一样做NLP、推荐系统，我会继续追问CRF和逻辑回归、最大熵模型的关系、CRF的优化方法、CRF和MRF的联系、HMM和CRF的关系（顺便问问朴素贝叶斯和HMM的联系，LSTM+CRF用于序列标注的原理、CRF的点函数和边函数、CRF的经验分布），wordembedding的几种常用方法和原理（顺便问问language model、perplexity评价指标、word2vec和glove的异同、topic model说一说、为何CNN能用在文本分类、syntactic和semantic问题举例、常见的sentence embedding方法、注意力机制（顺便问问注意力机制的几种不同情形，为何引入、seq2seq原理）、序列标注的评价指标、语义消岐的做法、常见的跟word有关的特征、factorization machine、常见的矩阵分解模型、如何把分类模型用于商品推荐（包括数据集划分、模型验证等）、序列学习、wide&deep model（顺便问问为何wide和deep）
2. 代码能力，主要考察事项算法和优化代码的能力，一般会先看面试者的github repo（如果简历给出来了），看代码风格、架构能力（遇到大神会认真学习一个），如果没有github，我会避免问典型的应试题，而是问一些我本人从实际问题中抽象处理的小算法题。
   - 给出节点的矩阵和边的矩阵，求路径和最大路径（来源于viterbi算法，本质就是个动态规划），至少给出思路和伪代码（顺便聊聊前向传播和后向传播）
   - 给出一个数组，数组元素是pair对儿，表示一个有向无环图<父节点，子节点>，用最优的方法，将其变成一个新的有序数组，数组元素是该有向无环图所有节点，数组的有序性体现在：父亲节点再孩子节点前面
3. 项目能力，主要考察解决问题的思路、填坑能力。这部分最考验面试官功底，要能从面试者浮夸的描述中寻找有意义的点，并一步步深挖。另外很多dirty work(数据预处理、文本清洗、调参经验、算法复杂度优化、Bad case分析、修改损失函数等)也是在这步深挖

### 归一化和标准化的区别？

- 归一化：
  - 把数变为(0,1)之间的小数，主要是为了数据处理方便提出来的，把数据映射到0-1范围之内处理，更方便快捷
  - 把有量纲的表达式变为无量纲表达式，归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
- 标准化
  - 把数据按比例缩放，使之落在一个小的指定区间。由于信用指标体系的各个指标度量单位是不同的，为了能够将指标参与评价计算，需要对指标进行规范化处理，通过函数变换将其数值映射到某个数值区间。

### 随机森林如何处理缺失值？

- 类别型变量用众数，连续型变量用中位数填补
- 用na.roughfix补上缺失值

### 随机森林如何评估特征重要性？

- decrease GINI
- decrease accuracy

### 解释对偶的概念

> 一个优化问题可以从两个角度进行考察，一个是primal问题，一个是dual问题，一般情况下，对偶问题给出主问题最优值的下界，再强对偶性成立的情况下，由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

### 如何进行特征选择？

>  特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使得模型泛化能力更强，减少过拟合，二是增强对特征和特征值之间的理解。

- 去除方差小的特征
- 正则化。L1正则化生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
- 随机森林。对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘法拟合。
- 稳定性选择。

### 数据预处理

1. 缺失值
2. 连续值离散化
3. 对定量特征二值化
4. 皮尔逊相关系数去除高度相关的列

### 特征工程

![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/feature.jpg)

### 对比Sigmoid、Tanh、Relu这三个函数

> [从ReLU到Sinc，26种神经网络激活函数可视化](https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ)
>
> [原文出处](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)
>
> Visualising Activation Functions in Neural Networks

- sigmoid函数又称为logistic函数，应用于logistic回归中。logistic回归的目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是从负无穷到正无穷。因此，使用logistic函数是将自变量映射到(0，1)上，映射后的值被认为是属于y=1的概率。
- sigmoid函数有如下几个缺点：
  - 正向计算包含指数，反向传播的导数也包含指数计算和除数计算，因而计算复杂度很高。
  - 输出的均值非零，这样使得网络很容易发生梯度消失和梯度爆炸，这是batch normalization要解决的问题。
- 对于ReLU来说，相对于sigmoid和tanh来说，有如下优点：
  - 收敛速度快，在实践中可以得知，他的收敛速度是sigmoid的6倍
  - ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合的问题
- ReLU的缺点是
  - 如果一个特别大的导数经过神经单元使得输入变得小于0，这样会使得这个单元永远得不到参数更新，因为输入小于0时，导数也是0。这样就形成了很多的dead cell。

### 既然上述三种激活函数各有优缺点，有没有更好的函数？

- Leaky ReLU

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/Leaky-ReLU.png)

- ELU

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/ELU.png)

- sigmoid

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/sigmoid1.png)

- tanh

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/tanh.png)

- ReLU

  ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/ReLU.png)

### 为什么引入非线性激励函数？

1. 对于神经网络而言，网络的每一层相当于$f(wx+b)=f(w^{'}x)$，对于线性函数，其实相当于$f(x)=x$，那么在线性激活函数下，每一层相当于用一个矩阵去乘x，那么多层就是反复的用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以在线性激励下，多层网络与一层网络相当。
2. 非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对问题空间进行简化，原来线性不可解的问题现在变得可解了。
3. 可以逼近任意函数

### 为什么ReLU要好过sigmoid/tanh函数？

1. sigmoid函数，用到指数运算，计算量大。方向传播时候求误差梯度时，求导涉及除法和指数运算，计算量相对大
2. 对于深层网络，sigmoid函数反向传播时候，很容易造成梯度消失的情况。在sigmoid函数接近饱和区时候，变换太缓慢了，导数趋于0，从而无法完成深层网络的训练
3. ReLU会使一步跟神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

> 现在主流的做法是，在每一层的输入都做一步batch normalization，尽可能使得每一层网络的输入具有相同的分布。

### 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？

- sigmoid用在各种gate上，产生0-1之间的数字，这个一般只有sigmoid最直接了
- tanh用在了状态和输出上，是对数据的处理，这个用其他激活函数也是可以的

### 衡量分类器的好坏？

> 这里首先知道TP、FN、FP、TN四种

- 精度precision=TP/(TP+FP)   == 认为正确的里面真正正确的
- 召回率recall=TP/(TP+FN)   == 全部正确的里面确实被认为是正确的
- F1值： 2/F1 = 1/recall + 1/precision
- ROC曲线：ROC空间是一个以伪阳率（FPR，false positive rate）为X轴，真阳率（TPR，true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP/P = recall， 伪阳率FPR=FP/N
- ![](https://github.com/spurscoder/spurscoder.github.io/raw/master/spurs/image/general/recall-precision.png)

## 03-30

## 03-31

## 04-01

## END

