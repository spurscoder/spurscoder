## 方言种类识别AI挑战赛		$\to\to\to$		top 10

### 1.赛题及时间

#### 1.1 赛题详情

方言种类识别 AI 挑战赛任务为汉语方言语言种类识别，即根据给定语音，判断该语音属于哪个方言。科大讯飞全球首次开放覆盖中国六大方言区，总时长约 60 小时的 10 种汉语方言语音数据集，供参加竞赛的科研单位以及开发者免费使用。

根据测试语音长度，方言识别 AI 挑战赛分为两个不同难度的子任务，即任务一（有效语音长度≤3s ）和任务二（有效语音长度>3s）。结果评价指标为分类正确率 acc：即分类正确的语音条数/所有语音条数。训练集合与开发集合供参加竞赛的科研单位以及开发者调试系统使用，测试集合不开放，最终排名以参赛者提交的系统在线上测试集合上的结果为准，分类正确率越高排名越靠前。

#### 1.2 开放数据

初赛共有六种方言，分别来源于六大方言区，具体为：长沙话（changsha）、河北话（hebei）、南昌话（nanchang）、上海话（shanghai）、闽南语（minnan）和客家话（kejia）。每种方言平均包含6小时的朗读风格语音数据，覆盖40个说话人。数据由各个型号的智能手机采集，录制环境包含安静环境和噪声环境。数据以采样率16000Hz，16比特量化的PCM格式存储。

数据集包含训练集、开发集和测试集三个部分。训练集每种方言有6000句语音，包含30个说话人，其中15位男性和15位女性，每个说话人200句语音；开发集和测试集分别每种方言包含5个说话人，其中开发集为2名女性和3名男性，测试集为3名女性和2名男性。开发集和测试集的数据根据语音段的时长分为两类，一类是小于等于3秒的短时数据（任务一），另一类是大于3秒的为长时数据（任务二），分别对应于两个比赛任务，其中每个说话人两类数据各50句，共100句。训练集、开发集、测试集的说话人均没有重复。数据具体描述见表1。

为了增加本次比赛技术方案的多样性，每条语音对应文本内容的音素序列标注也将同样提供。

#### 1.3 时间限

★方言种类识别AI挑战赛：
1. 报名时间：3月22日——6月19日
2. 初赛阶段：6月20日——7月19日
3. 复赛阶段：7月29日——9月19日
4. 决赛团队支持辅导：7月29日——9月19日
5. 决赛及颁奖：10月24日（全球1024开发者节）

小助手在这里恭候各位大神的作品！
注：
①各位选手如果有参赛队友需拉入群组，可以私聊小助手，小助手审核无误后，会将您的队友一起拉入群组哈~
②请各位选手进群后改下自己的备注名+公司/学校名称（如：张三+中国科学技术大学），谢谢合作！

### 2. 资料查询

#### 2.1 Github中的repository

##### 2.1.1 [Deep Audio Classification](https://github.com/despoisj/DeepAudioClassification)

> Finding the genre of a song with Deep Learning

![](https://github.com/despoisj/DeepAudioClassification/raw/master/img/pipeline.png)

- 总结：*将音频转换为图片，对图片进行裁剪之后，用CNN等进行分类。*

##### 2.1.2 [Audio Classification](https://github.com/ogencoglu/AudioClassification)

>  Data (audio files) are not pushed. Contact me if you need them. If you want to train with your own data, create a directory *data* under home having two subdirectories under it, i.e. *breathing* and *non_breathing*. Put your audio files or directories containing the audio files under the corresponding class.

- To Do
  - Hyperparameter Optimization
  - Upsample/downsample each input according to fixed sampling rate
  - (Optional) Implement scikit-learn-independent testing
- 总结：*用sklearn中的SVC，简单的分类。没有太多的优化和预处理*

##### 2.1.3 [Audio Music Mood Classification](https://github.com/neokt/audio-music-mood-classification) 

> Classification using audio features to generate a mood profile for a [spotify](https://www.spotify.com/) playlist. Analysis and modeling done with pandas, scikit-learn ensemble methods, and XGBoost. Visualizations created using matplotlib and seaborn. Production in Flask. Data queried from Billboard API, Spotify API and Gracenote API.

- 总结：coder分析了[Spotify: Music for everyone](https://www.spotify.com/)中的歌曲，对其进行分类

![](https://raw.githubusercontent.com/neokt/neokt.github.io/master/images/audio-music-mood-classification_chart1.png)



##### 2.1.4 [Multiclass audio segmentation using `auditok` and GMMs](http://nbviewer.jupyter.org/github/amsehili/audio-segmentation-by-classification-tutorial/blob/master/multiclass_audio_segmentation.ipynb)

- What is audio segmentation?

  Audio segmentation means at least two things:

  - Figure out, within an audio stream (audio file, network stream, audio device, etc.), regions that represent an **audio activity** (no matter what kind of activity) and those that represent a **silence**. This type of segmentation is rather referred to as **Audio (or Acoustic) Activity Detection** (**AAD**) and is a binary classification problem.
  - Tell apart, within an audio stream, the nature of **audio activities** (e.g. speech, engine, bird, glass break, etc.). This is a multi-class classification problem.


- 总结：*对一段断断续续的语音进行分割之后，分类。*用到了GMM
- 对应的[github](https://github.com/amsehili/audio-segmentation-by-classification-tutorial)



##### 2.1.5 [audio classification using TensorFlow](https://github.com/MasazI/audio_classification) 

数据集：[UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html)

> audio classification is an implementation MLP for audio classification using TensorFlow

- requirements
  - TensorFlow 0.10+
  - Numpy
  - Matplotlib
  - Librosa


- 总结：很有价值，coder直接使用了tensorflow，文档简洁清晰。CNN+FC+BN
- 使用了waveplot，频谱，log频谱
- [audio wave image](https://github.com/MasazI/audio_classification#audio-wave-image)

##### 2.1.6 [Audio Classification](https://github.com/nextco/audio-classification) - Multilayer Neural Networks using TensorFlow 

数据集：[UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html)

- 采样并处理成了三种谱

  ![](https://github.com/nextco/audio-classification/raw/master/img/sampling.gif)

  ![](https://github.com/nextco/audio-classification/raw/master/img/plot-single.png)


- 总结：直接DNN


##### 2.1.8 [Deep Learning experiments for audio classification](https://github.com/jaron/deep-listening) 

数据集：[UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html)

- 总结：分别用DNN(3 layers)， CNN，RNN
- [对应的三种模型的详细解释](https://github.com/jaron/deep-listening#deep-listening)

##### 2.1.9 [python_speech_features](https://github.com/jameslyons/python_speech_features)

>  This library provides common speech features for ASR including MFCCs and filterbank energies.

- [github1](https://github.com/begeekmyfriend/ezfm_diarisation) 用到了这个包提取特征，做自己的实验


- 总结：提供了提取特征的函数。

##### 2.1.10 [pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis)

> Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications

- 总结：不太懂这个


---

> 以下是github上一些比较新的audio classification repositories

##### 2.1.11 [Speech-Signal-Processing-and-Classification](https://github.com/manashmndl/DeadSimpleSpeechRecognizer)

- Feature extraction techniques implemented in Python：

  MFCC、LPC、PLP、MGCA

- Classifiers implementation , initial step for gender and then for healthy or not cases.

  - Gaussian Mixture Models
  - K-nearest Neighbours
  - Logistic Regression
  - Support Vector Machine
  - Linear Discriminant Analysis
  - Decision Tree Classifier
  - GaussianNB
  - Neural Networks












#### 2.2 音频数据集

##### 2.2.1  [UrbanSound](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound.html) and [UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html) datasets

Welcome to the companion site for the [UrbanSound](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound.html) and [UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html) datasets and the [Urban Sound Taxonomy](https://serv.cusp.nyu.edu/projects/urbansounddataset/taxonomy.html). Here you will find information and download links for the datasets and taxonomy presented in:

*J. Salamon, C. Jacoby and J. P. Bello, "**[A Dataset and Taxonomy for Urban Sound Research](http://serv.cusp.nyu.edu/projects/urbansounddataset/salamon_urbansound_acmmm14.pdf)**", 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.*

- 总结：估计有很多再此数据集上的论文。



##### 2.2.2 [ESC-50](https://github.com/karoldvl/ESC-50): Dataset for Environmental Sound Classification

> The **ESC-50 dataset** is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.
>
> The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:
>
> ![](https://github.com/karoldvl/ESC-50/raw/master/esc50.gif)

- 总结：*该主页下方的[result](https://github.com/karoldvl/ESC-50#results)，是在该数据集上的论文成果列表，值得一读*。

  | 例如：还有很多                                  |                                          |          |                                          |      |
  | ---------------------------------------- | ---------------------------------------- | -------- | ---------------------------------------- | ---- |
  | Title                                    | Notes                                    | Accuracy | Paper                                    | Code |
  | **Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification** | CNN with filterbanks learned using convolutional RBM + fusion with GTSC and mel energies | 86.50%   | [sailor2017](https://pdfs.semanticscholar.org/f6fd/1be38a2d764d900b11b382a379efe88b3ed6.pdf) |      |
  | **Learning from Between-class Examples for Deep Sound Recognition** | EnvNet-v2 ([tokozume2017a](http://www.mi.t.u-tokyo.ac.jp/assets/publication/LEARNING_ENVIRONMENTAL_SOUNDS_WITH_END-TO-END_CONVOLUTIONAL_NEURAL_NETWORK-poster.pdf)) + data augmentation + Between-Class learning | 84.90%   | [tokozume2017b](https://openreview.net/forum?id=B1Gi6LeRZ) |      |
  | **Novel Phase Encoded Mel Filterbank Energies for Environmental Sound Classification** | CNN working with phase encoded mel filterbank energies (PEFBEs), fusion with Mel energies | 84.15%   | [tak2017](https://www.researchgate.net/profile/Dharmesh_Agrawal/publication/320733074_Novel_Phase_Encoded_Mel_Filterbank_Energies_for_Environmental_Sound_Classification/links/5a084c780f7e9b68229c8947/Novel-Phase-Encoded-Mel-Filterbank-Energies-for-Environmental-Sound-Classification.pdf) |      |









#### 2.3 博客及论文

##### 2.3.1 STFT和声谱图，梅尔频谱（Mel Bank Features）与梅尔倒谱（MFCCs）](https://blog.csdn.net/qq_28006327/article/details/59129110)

> 最近小编在做ASC（Acoustic Scene Classification）问题，不管是用传统的GMM模型，还是用机器学习中的SVM或神经网络模型，提取声音特征都是第一步。梅尔频谱和梅尔倒谱就是使用非常广泛的声音特征形式，小编与它们斗争已有一段时间了，在此总结一下使用它们的经验

- 文中提到了`from scipy import signal`处理音频

- 还有工具`import librosa`

  > 专业的音频处理工具包

- 总结：*本博客提到了如何将音频预处理，装换为各种谱图。其中，梅尔频谱和梅尔倒谱用的比较多*

##### 2.3.2 基于SVM的音频分类系统设计及实现

2010年论文，[地址](http://www.jsjkx.com/jsjkx/ch/reader/create_pdf.aspx?file_no=101249&flag=1&journal_id=jsjkx&year_id=2010)

##### 2.3.3 Building a Dead Simple Speech Recognition Engine using ConvNet in Keras

[地址](https://blog.manash.me/building-a-dead-simple-word-recognition-engine-using-convnet-in-keras-25e72c19c12b)

##### 2.3.4 [DeadSimpleSpeechRecognizer](https://github.com/manashmndl/DeadSimpleSpeechRecognizer)

> CNN based minimal model for recognizing word

- 总结：将wav转换为mfcc之后，用CNN处理








#### 2.4 通常的处理思路总结



### 3. 工具及相关教程

#### 3.1 librosa - Python library for audio and music analysis 

> [官方教程](http://librosa.github.io/librosa/index.html#)
>
> [音频特征提取——librosa工具包使用](http://www.cnblogs.com/xingshansi/p/6816308.html)
>
> [github](https://github.com/librosa/librosa)

#### 3.2 Pysox - Python wrapper around *sox*.

> [官方教程](http://pysox.readthedocs.io/en/latest/)
>
> [github](http://pysox.readthedocs.io/en/latest/)

- [Sox](http://sox.sourceforge.net/)

  > SoX is a cross-platform (Windows, Linux, MacOS X, etc.) command line utility that can convert various formats of computer audio files in to other formats. It can also apply various effects to these sound files, and, as an added bonus, SoX can play and record audio files on most platforms.

#### 3.3 eyeD3 is a Python module and command line program for processing ID3 tags

> [官方教程](https://eyed3.readthedocs.io/en/latest/)
>
> [github](https://github.com/nicfit/eyeD3)
>
> eyeD3 is a Python module and command line program for processing ID3 tags. Information about mp3 files (i.e bit rate, sample frequency, play time, etc.) is also provided. The formats supported are ID3v1 (1.0/1.1) and ID3v2 (2.3/2.4).

#### 3.4 [FFmpeg](https://ffmpeg.org)  - A complete, cross-platform solution to record, convert and stream audio and video.

> Converting **video** and **audio** has never been so easy.

#### 3.5 pydub音频处理库使用详解

> Pydub lets you do stuff to audio in a way that isn't stupid.
>
> [github](https://github.com/jiaaro/pydub)
>
> [官方文档](https://github.com/jiaaro/pydub/blob/master/API.markdown)
>
> [pydub音频处理库使用详解](https://xin053.github.io/2016/11/05/pydub%E9%9F%B3%E9%A2%91%E5%A4%84%E7%90%86%E5%BA%93%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/)