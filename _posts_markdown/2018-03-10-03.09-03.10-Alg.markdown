---
layout: post
title: "03.09到03.10.algorithm"
author: "Spurs"
date: 2018-03-10 12:00:00
tags:
  - dinary
  - everyday
---

[TOC]

# algorithm

## 2018-03-07

### 1. QuickSort

- 时间复杂度：O(nlgn),最坏O(n^2)。

- 空间复杂度：O(nlgn)

- 步骤：

  1. 分解：把A[p…r] 分解为两个数组A[p…q-1] 和A[q+1…r],使得A[q-1] <= A[q] <= A[q+1]
  2. 解决：分别对A[p…q-1]和A[q+1…r]进行排序
  3. 合并

- 伪代码：

  ```c++
  // QuickSort
  if p < r:
  	then q <- Partition(A, p, r):
  		QuickSort(A, p, q-1)
          QuickSort(A, q+1, r)
              
  // Partition
  x <= A[r]
  i <= p-1
  for j<=p to r-1:
  	do if A[j] <= x:
  		then i <= i+1:
  			swap(A[i]<=>A[j])
  swap(A[i+1]<=>A[r])
  return i + 1
  ```

- qsort1

  ```c++
  void quicksort(table *tab, int left, int right)
  {
      int i, j;
      if (left < right){
          i = left; j = right;
          tab->r[0] = tab->r[i];
        do 
          {
            while(tab->r[j].key > tab->r[0].key && i < j)
              j--;
            if (i < j)
              {
                tab->r[i].key = tab->r[j].key;
                i++;
              }
          	while(tab->r[i].key <= tab->r[0] && i < j)
                i++
                if (i < j)
                  {
                    tab->r[j] = tab->r[i];
                  	j--;
                  }
          }while(i!=j)
          tab->r[i] = tab->r[0]
          quicksort(tab, left, i-1);
        	quicksort(tab, i+1, right);
      }
  }
  ```

- Hoare版本的快排

  ```
  void QuickSort(int data[], int lo, int hi)
  {
    int i,j,temp;
    temp = data[lo];
    i = lo; j = hi;
    if (lo > hi) return;
    while (i != j) {
      while (data[j] >= temp && j > i)
      	j--;
      if (j > i)
     		data[i++] = data[j]
      while (data[i] <= temp && j > i)
      	i++;
      if (j > i)
      	data[j--] = data[i];
    }
    data[i] = temp;
    QuickSort(data, lo, i-1);
    QuickSort(data, i+1, hi);
  }
  ```

  非递归版本的quickSort

  ```
  template <class T>
  int RandPartition(T data[], int lo, int hi)
  {
    T v = data[lo];
    while(lo < hi) {
      while(lo < hi && data[hi] > v)
      	hi --;
      data[lo] = data[hi];
      while(lo < hi && data[lo] <= v)
      	lo ++;
      data[hi] = data[lo];
    }
    data[lo] = v;
    return lo;
  }

  template <class T>
  void quickSort(T data[], int lo, int hi) {
    stack<int> st;
    int key;
    do
    {
      while(lo < hi) 
      {
        key = partition(data, lo, hi);
        // 递归的本质是什么？ 就是借助栈，实现进栈出栈的。
        if ((key-lo) < (key-hi))
        {
          st.push(key+1)；
          st.push(hi);
          hi = key - 1;
        }
        else
        {
          st.push(lo);
          st.push(key-1);
          lo = key + 1;
        }
      }
      if (st.empty())
      	return ;
      hi = st.top();
      st.pop();
      lo = st.top();
      st.pop();
    }while(1);
  }

  void QuickSort(int data[], int lo, int hi)
  {
    if (lo < hi) {
      int k = RandPartition(data, lo, hi);
      QuickSort(data, lo, k-1);
      QuickSort(data, k+1, hi);
    }
  }
  ```

- ```c++
  // this is the important code
  class Solution{
  public:
  	ListNode* quickSort(ListNode *head) {
  		if (head == NULL || head->next == NULL) return head;
  		qsort(head, NULL);
  		return head;
  	}
  	void qsort(ListNode* head, ListNode *tail) {
  		if (head != tail && head->next != tail) {
  			ListNode* mid = partitionList(head, tail);
  			qsort(head, mid);
  			qsort(mid->next, tail);
  		}
  	}
  	ListNode* partitionList(ListNode* head, ListNode* tail) {
  		int key = head->val;
  		ListNode* loc = head;
  		for(ListNode* i = head->next; i != tail; i = i->next) {
  			if (i->val < key) {
  				loc = loc->next;
  				swap(i->val, loc->val);
  			}
  		}
  		swap(loc->val, head->val);
  		return loc;
  	}
  }
  ```

### 2. InsertSort

- 链表下的插入排序

  ```c++
  class Solution{
    ListNode* insertionSortList(ListNode* Node) {
      if (head == NULL || head->next == NULL) return head;
      ListNode* p = head->next, *pstart = new ListNode(0), *pend = head;
      pstart->next = head;
      while(p) {
      	ListNode* tmp = pstart->next, *pre=pstart;
      	while(tmp != p && p->val >= tmp->val)
      	{
            tmp = tmp->next;
            pre = pre->next;
      	}
      	if (tmp == p) pend = p
      	else
      	{
            pend->next = p->next;
            p->next = tmp;
            pre->next = p;
      	}
      	p = pend->next;
  	}
  	head = pstart->next;
  	delete pstart;
  	return head;
    }
  }
  ```

### 3. BubbleSort

* 链表下的冒泡排序

  ```c++
  class Solution{
  public:
  	ListNode* bubbleSortList(ListNode *head) {
  		if (head == NULL || head->next == NULL) return head;
  		ListNode *p = NULL;
  		bool isChange = true;
  		while(p != head->next && isChange) {
  			ListNode *q = head;
  			isChange = false;
  			for(; q->next && q->next != p; q = q->next) {
  				if (q->val > q->next->val) {
  					swap(q->val, q->next->val);
  					isChange = true;					
  				}
  			}
  			p = q;
  		}
  		return head;
  	}
  }
  ```

### 4. MergeSort

- 归并排序应该算是链表排序中最佳的选择了

- ```c++
  class Solution{
  public:
  	ListNode* mergeSortList(ListNode *head) {
  		if (head == NULL || head->next == NULL) return head;
  		else {
  			// kuai man zhi zhen zhaodao zhongjian jiedian 
  			ListNode *fast = head, *slow = head;
  			while(fast->next != NULL && fast->next->next != NULL) {
  				fast = fast->next->next;
  				slow = slow->next;
  			}
  			fast = slow;
  			slow = slow->next;
  			fast->next = NULL;
  			fast = mergeSortList(head);
  			slow = mergeSortList(slow);
  			return merge(fast, slow);
  		}
  	}
  	// merge two sorted list to one
  	ListNode* merge(ListNode *head1, ListNode *head2) {
  		if (head1 == NULL) return head2;
  		if (head2 == NULL) return head1;
  		ListNode *res, *p;
  		if (head1->val < head2->val) {
  			res = head1; head1 = head1->next;
  		} else {
  			res = head2; head2 = head2->next;
  		}
  		p = res;

  		while(head1 && head2) {
  			if (head1->val > head2->va) {
  				p->next = head2;
  				head2 = head2->next;
  			} else {
  				p->next = head1;
  				head1 = head1->next;
  			}
  		}
  		if (head1) p->next = head1;
  		else p->next = head2;
  		return res;
  	}
  }
  ```


## 2018-03-08

### 1.处理海量数据 - 分而治之 

> 密钥一：
>
> ​	分而治之/Hash映射 + Hash_map统计 + 快速/堆/归并排序

1. 问题：从海量日志数据中，提取出某日访问百度次数最多的ip
   1. 无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序。
   2. 首先将IP从日志中提取出来，其中最多有2^32个IP。对其进行取模%1000，映射为小文件，
   3. 对每个小文件进行hash_map统计，之后对map进行排序得到访问次数最多的IP。
   4. 然后对1000个IP进行排序，得到频率最大的IP。

2. 问题：寻找热门查询，300万个查询字符串中统计最热门的10个查询。

   > 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。
   >
   > Solution1: 使用上述方法。分而治之之后hash_map统计。
   >
   > Solution2: 或许可以直接加载到内存中

   - 如何确定可以记载到内存中？
     - 需要一个合适的数据结构，hashtable is good。
     - 3M * 255 = 0.75G < 1G
   - 具体过程
     - hash_map 统计，先对数据进行预处理。具体方法就是维护一个字典{key, value},key就是我们的字符串，value就是字符串出现的具体次数。时间复杂度为O(N)
     - 堆排序：因为我们需要找到最大的k个查询，我们使用最小堆这个数据结构，建立一个K大小的堆，需要O(k)时间建堆，之后对其进行调整log(k)，之后遍历所有的300W的字符串，与最小堆的堆顶进行比较。时间复杂度为O(N'log(k))
     - 所以总的时间复杂度为O(N) + O(N'log(k))
   - 补充：可以用trie树替换hash_table来统计字符串出现的次数。

3. 问题：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

   > 类似于上面两个例题，同样的分而治之，同样的统计，同样的归并。

   - 分而治之/hash映射：顺序读文件，对于每个词x，取hash(x)%5000, 然后按照该值存到5000个小文件中，这样大概每个文件大小为200k，如果某个文件大于1M，则可以按照上述方法继续分支。
   - hash_map统计：对于每个小文件采用trie树/hash_map等统计文件中出现的每个词的频率。
   - 堆/归并排序：取出频率每个文件中频率最大100个词，存储到各个文件中，这样得到了5000个文件，对其进行归并排序/堆排序

4. 问题：海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

   > 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素

   - 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成(TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）
   - 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。

   > 如果每个数据元素不止出现在某一台机器中，那么可以采用以下两种方法：
   >
   > 1. 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。
   > 2. 暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。

5. 问题：有十个文件，每个文件1G，每个文件中每一行保存了用户的query，每个文件中的query可能重复，请对每个query按照其频度排列。

   - 方案一：

     1. hash映射：顺序读取十个文件，通过hash(query)%10将query映射到另外的十个文件中，新的十个文件，每个文件大小大约1G（假设hash是随机的）
     2. hash_map统计：对每个文件进行hash_map(query, query_count)来统计query出现的次数。
     3. 堆/快速/归并排序：首先对每个文件进行排序，会得到十个排好序的文件，之后通过归并排序对十个文件进行排序，内排序和外排序相结合。

   - 方案二：

     一般query的总量是有限的，只是重复的次数多而已，或许可以一次性读入内存。如果可以读入内存，我们可以通过使用trie树/hash_map直接对其进行排序，然后按照出现的次数快速/堆/归并排序。

   - 方案三：

     与方案一的方式近似，hash映射之后，得到新的十个文件，可以通过十个线程来处理，采用分布式架构（mapreduce），最后再进行合并。[代码实现](https://github.com/adbmal/sortquery/blob/master/querysort.py)

6. 问题：给定两个文件a，b，各存放50亿url，每个url64字节，内存限制4G，找出a，b中相同的url

   > 分析：every file‘s count is 5G * 64 = 320G larger than 4G, so we need conquer and handle it.

   1. 分而治之/hash映射：分别遍历文件a，b，对每个url取hash(url)%1000，将最终结果映射到1000个文件中，每个文件大小约为300M，内存可容。因为对于两个文件的子文件来说，不同的hash，肯定不会有相同的url，所以我们只需要比较相同hash值的两个文件即可。
   2. hash_set统计：对ai进行hash_set存储，此时遍历bi，判断bi中每个url是否在hash_set中，从而判断是否有相同的url

7. 问题：怎么在海量数据中找出重复次数最多的一个

   > 分而治之，先hash映射为小文件，之后对每个文件进行统计，最后得到重复次数最多的一个。

8. 问题：上万或者上千亿的数据（有重复），统计重复次数最多的前N的数据

   > 用hash_map/红黑树等进行统计，然后利用堆来取出前N的数据。

9. 问题：一个文本文件有一万行，每行一个词，要求统计出其中重复次数最多的词

   > 分而治之

10. 问题：一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。

   > 分而治之

11. 问题：找出100W个数中最大的100个数

    - 方案一：

      利用局部淘汰法，取前100个数进行排序，记为L，再遍历剩余的树，每次都与L中最小的数比较，如果大于Lmin，则淘汰Lmin之后将数字插入到L中,复杂度为O(100W*100)

    - 方案二：

      采用快速排序的方法，每次只考虑比轴大的地方，直到比轴大的一部分比100多的时候，采用传统的方法排序，取出前100个，时间复杂度O(100W*100)

    - 方案三：

      最小堆，时间复杂度为O(100W*log(100))

12. 问题：1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

    - 方案一：使用trie树或者hash_map可以解决问题
    - 方案二：使用set。？？

## 2018-03-09

### 2.处理海量数据 - 多层划分

> 多层划分----其实本质上还是分而治之的思想，重在“分”的技巧上！
> 　　适用范围：第k大，中位数，不重复或重复的数字
> 　　基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。

1. 问题：2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

   >  有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。

2. 问题：5亿个int找它们的中位数

   - 思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。
     ​	实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。

### 3.处理海量数据 - Bloom filter / Bitmap

- [bloom filter](http://blog.csdn.net/v_july_v/article/details/6685894) *我还没太花时间看，不太清楚是如何使用*

  > 适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集
  > 　　基本原理及要点：
  > 　　对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。
  > 　　还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数k=(ln2)*(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于n*lg(1/E)才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该>=nlg(1/E)*lge 大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。
  > 　　举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。
  > 　　注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。

  > **“6、给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？**
  >
  > 　　根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。
  >
  > ​    同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。”

- [Bitmap](http://blog.csdn.net/v_july_v/article/details/6685962):

  1. 问题：**在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。**
     - 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。
     - 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。
  2. 问题：**给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？**
     - 方案1：frome oo，用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

### 4.处理海量数据 - trie树/数据库/倒排缩阴

- [Trie树](http://blog.csdn.net/v_july_v/article/details/6897097) 

  > 适用范围：数据量大，重复多，但是数据种类小可以放入内存
  > 　　基本原理及要点：实现方式，节点孩子的表示方式
  > 　　扩展：压缩实现。

  1. 适用实例：
     1. 上面的**第2题**：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。
     2. 上面的**第5题**：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。
     3. 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？
     4. 上面的**第8**题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。

- 数据库

  **数据库索引**
  　　适用范围：大数据量的增删改查
  　　基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。

  - 关于数据库索引及其优化，更多可参见此文：<http://www.cnblogs.com/pkuoliver/archive/2011/08/17/mass-data-topic-7-index-and-optimize.html>；
  - 关于MySQL索引背后的数据结构及算法原理，这里还有一篇很好的文章：<http://blog.codinglabs.org/articles/theory-of-mysql-index.html>；
  - 关于B 树、B+ 树、B* 树及R 树，本blog内有篇绝佳文章：<http://blog.csdn.net/v_JULY_v/article/details/6530142>。

- 倒排索引

  **倒排索引(Inverted index)**
  　　适用范围：搜索引擎，关键字查询
  　　基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。
  　以英文为例，下面是要被索引的文本：
  ​    T0 = "it is what it is"
  ​    T1 = "what is it"
  ​    T2 = "it is a banana"
  ​    我们就能得到下面的反向文件索引：
  ​    "a":      {2}
  ​    "banana": {2}
  ​    "is":     {0, 1, 2}
  ​    "it":     {0, 1, 2}
  ​    "what":   {0, 1}
  　检索的条件"what","is"和"it"将对应集合的交集。
  　　正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。
  　　扩展：
  　　问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。

  ​    关于倒排索引的应用，更多请参见：

  - [第二十三、四章：杨氏矩阵查找，倒排索引关键词Hash不重复编码实践](http://blog.csdn.net/v_july_v/article/details/7085669)，
  - [第二十六章：基于给定的文档生成倒排索引的编码与实践](http://blog.csdn.net/v_july_v/article/details/7109500)。

### 5.处理海量数据 - 外排序

> 适用范围：大数据的排序，去重
>
> 基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树

​	问题实例：
　　1).有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。
　　这个数据具有很明显的特点，词的大小为16个字节，**但是内存只有1M做hash明显不够**，所以可以用外排序。内存可以当输入缓冲区使用。

​    关于多路归并算法及外排序的具体应用场景，请参见blog内此文：

- [第十章、如何给10^7个数据量的磁盘文件排序](http://blog.csdn.net/v_JULY_v/archive/2011/05/28/6451990.aspx)

### 6.处理海量数据 - 分布式处理之Mapreduce

>  MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。

> 适用范围：数据量大，但是数据种类小可以放入内存

>  基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。

​	问题实例：

1. The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents:
2. 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
3. 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？

​    更多具体阐述请参见blog内：

- [从Hadhoop框架与MapReduce模式中谈海量数据处理](http://blog.csdn.net/v_july_v/article/details/6704077)，
- 及[MapReduce技术的初步了解与学习](http://blog.csdn.net/v_july_v/article/details/6637014)。

## 2018-03-10





## end

